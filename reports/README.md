# Spam Filter Plots

This folder contains static PNGs and interactive HTML exports generated by `src/plot.py`. They visualize the CSV outputs from the experiments (`learning_curves.csv`, `hyperparameter_results.csv`).

## Learning curve views
- `learning_curves.[png|html]`: 2×3 grid (accuracy, precision, recall, F1, FPR, FNR) across classifiers × window sizes. Use it to spot over/under-fitting trends as training data grows.
- `learning_curve_<metric>.[png|html]`: Single-metric slices for quick inspection or embedding.
- `learning_curves_focused.[png|html]`: Only window=200, comparing classifiers on accuracy/precision/recall/F1 for a cleaner head-to-head view.
- `learning_curves_all_in_one.[png|html]`: Overlays the four key metrics in one panel for window=200; handy for quick sanity checks.

### Current takeaways from `learning_curves.csv`
- Feature Hashing (window=200) finishes around precision≈0.95, recall≈0.80, F1≈0.87, FPR≈0.06, FNR≈0.20 at 254k examples.
- Count-Min (window=200) trails on F1≈0.83 with lower FPR≈0.04 but higher FNR≈0.28.
- Larger windows (500–2000) give similar F1 for Feature Hashing; gains appear to plateau, suggesting diminishing returns beyond ~200 window.
- Precision stays above recall for both models, so thresholds skew toward avoiding false positives more than maximizing spam capture.

## Hyperparameter exploration
- `hyperparameter_heatmap_fh.[png|html]`: Feature Hashing heatmaps showing precision/recall/F1/accuracy across `ngram` vs `log_buckets` (final step per config).
- (Count-Min heatmaps/HTML are not present in this drop; rerun plotting after regenerating `hyperparameter_results.csv` if needed.)

### Current takeaways and cautions
- The stored hyperparameter CSV looks truncated: many `fnr` entries are missing and F1 values cap around 0.18. Treat the heatmaps as indicative only and rerun experiments before making tuning decisions.
- With the available rows, lower `log_buckets` and unigram settings dominate the top of the (partial) table; expect this to change with complete data.

## Using the artifacts
- PNGs are quick to review or embed in docs; HTML files allow hover-tooltips and toggling traces.
- When comparing variants, keep FPR/FNR alongside F1 to balance user experience (false alarms) against missed spam.
- If new CSVs are generated, rerun `src/plot.py` to refresh both PNG and HTML files in this folder.

## Verification snapshot
- Data volume: learning curves reach 254k examples; curves are smooth and monotonically improving, matching expectations for incremental Naive Bayes training.
- Best observed config so far: Feature Hashing, window=200 (F1≈0.87) with low FPR; Count-Min offers lower FPR but meaningfully lower recall.
- Anomalies: hyperparameter results are incomplete (missing FNR, low F1 ceiling); Count-Min heatmaps absent from artifacts—regenerate before relying on tuning insights.
