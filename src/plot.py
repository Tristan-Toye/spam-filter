#!/usr/bin/env python3
"""
Visualization script for spam filter experiments.
Reads CSV files generated by the C++ experiments and creates plots
for the SELECTED configurations only (not cross-validation data).
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
from pathlib import Path
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots

# Set style for better-looking plots
sns.set_style("whitegrid")
sns.set_palette("husl")
plt.rcParams['figure.figsize'] = (12, 8)
plt.rcParams['font.size'] = 10

OUTPUT_DIR = Path("reports")
OUTPUT_DIR.mkdir(exist_ok=True)

# Baseline precision for spam filtering (proportion of spam in dataset)
BASELINE_PRECISION = 0.609205


def _save_png(fig, name: str):
    """Save matplotlib figure to reports/."""
    fig.savefig(OUTPUT_DIR / name, dpi=300, bbox_inches='tight')
    plt.close(fig)


def _save_html(fig, name: str):
    """Save plotly figure to reports/ with full-screen responsive layout."""
    # Update layout for full-width, responsive sizing with proper spacing
    fig.update_layout(
        autosize=True,
        margin=dict(l=60, r=60, t=80, b=60),  # Generous margins
    )
    
    # Custom HTML with responsive styling
    html_content = f'''<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>{name.replace('.html', '').replace('_', ' ').title()}</title>
    <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
    <style>
        * {{ margin: 0; padding: 0; box-sizing: border-box; }}
        html, body {{ 
            width: 100%; 
            height: 100%; 
            overflow-x: hidden;
        }}
        .plotly-graph-div {{ 
            width: 100% !important; 
            height: 100vh !important;
            min-height: 700px;
        }}
    </style>
</head>
<body>
    <div id="plotly-div"></div>
    <script>
        var figure = {fig.to_json()};
        figure.layout.autosize = true;
        figure.layout.height = null;
        figure.layout.width = null;
        Plotly.newPlot('plotly-div', figure.data, figure.layout, {{responsive: true}});
        
        // Resize handler
        window.addEventListener('resize', function() {{
            Plotly.Plots.resize('plotly-div');
        }});
    </script>
</body>
</html>'''
    
    with open(OUTPUT_DIR / name, 'w', encoding='utf-8') as f:
        f.write(html_content)


def _config_label(row, include_opt=True):
    """Create a descriptive label for a configuration."""
    clf = row['classifier']
    ng = int(row['ngram'])
    lb = int(row['log_buckets'])
    nh = int(row['num_hashes']) if 'num_hashes' in row else 0
    
    if clf == 'FeatureHashing':
        label = f"FH: ng={ng}, lb={lb}"
    else:
        label = f"CM: ng={ng}, lb={lb}, nh={nh}"
    
    if include_opt and 'optimized_for' in row:
        label += f" ({row['optimized_for']})"
    
    return label


def plot_pr_curves_selected():
    """
    Plot PR curves from Phase 3 threshold tuning for selected configurations.
    Creates combined PR curve and F0.5 vs threshold plot.
    """
    print("Plotting PR curves for selected configurations...")
    
    try:
        df = pd.read_csv('pr_curve.csv')
    except FileNotFoundError:
        print("  Warning: pr_curve_phase3.csv not found")
        return
    
    # Get unique configurations
    config_cols = ['classifier', 'ngram', 'log_buckets', 'num_hashes', 'optimized_for']
    configs = df.groupby(config_cols).size().reset_index()[config_cols]
    
    print(f"  Found {len(configs)} configurations")
    
    # Color palettes for classifiers
    fh_configs = configs[configs['classifier'] == 'FeatureHashing']
    cm_configs = configs[configs['classifier'] == 'CountMin']
    fh_colors = plt.cm.Blues(np.linspace(0.4, 0.9, max(len(fh_configs), 1)))
    cm_colors = plt.cm.Greens(np.linspace(0.4, 0.9, max(len(cm_configs), 1)))
    
    # === Matplotlib: Combined PR and F0.5 plots ===
    fig, axes = plt.subplots(1, 2, figsize=(16, 7))
    
    fh_idx, cm_idx = 0, 0
    
    for _, config in configs.iterrows():
        mask = True
        for col in config_cols:
            mask = mask & (df[col] == config[col])
        df_config = df[mask].copy()
        
        if len(df_config) == 0:
            continue
        
        label = _config_label(config)
        
        if config['classifier'] == 'FeatureHashing':
            color = fh_colors[fh_idx % len(fh_colors)]
            fh_idx += 1
        else:
            color = cm_colors[cm_idx % len(cm_colors)]
            cm_idx += 1
        
        # PR curve
        axes[0].plot(df_config['recall'], df_config['precision'],
                    linewidth=1.5, label=label, alpha=0.8, color=color)
        
        # F0.5 vs threshold
        axes[1].plot(df_config['threshold'], df_config['f05'],
                    linewidth=1.5, label=label, alpha=0.8, color=color)
    
    # Add baseline to PR curve
    axes[0].axhline(y=BASELINE_PRECISION, color='red', linestyle='--', 
                   linewidth=1.5, alpha=0.7, label=f'No-skill (P={BASELINE_PRECISION:.1%})')
    
    # Format PR curve
    axes[0].set_xlabel('Recall (True Positive Rate)')
    axes[0].set_ylabel('Precision')
    axes[0].set_title('PR Curves - Selected Configurations')
    axes[0].legend(loc='lower left', fontsize=7, ncol=2)
    axes[0].grid(True, alpha=0.3)
    axes[0].set_xlim([0, 1])
    axes[0].set_ylim([0, 1])
    
    # Format F0.5 plot
    axes[1].set_xlabel('Threshold')
    axes[1].set_ylabel('F0.5 Score')
    axes[1].set_title('F0.5 vs Threshold - Selected Configurations')
    axes[1].legend(loc='lower right', fontsize=7, ncol=2)
    axes[1].grid(True, alpha=0.3)
    
    plt.tight_layout()
    _save_png(fig, 'pr_curves_selected.png')
    print("  Saved: reports/pr_curves_selected.png")
    
    # === Plotly: Interactive version ===
    fig_html = make_subplots(rows=1, cols=2,
                             subplot_titles=["PR Curves", "F0.5 vs Threshold"],
                             horizontal_spacing=0.1)
    
    for _, config in configs.iterrows():
        mask = True
        for col in config_cols:
            mask = mask & (df[col] == config[col])
        df_config = df[mask].copy()
        
        if len(df_config) == 0:
            continue
        
        label = _config_label(config)
        
        # PR curve
        fig_html.add_trace(
            go.Scatter(x=df_config['recall'], y=df_config['precision'],
                      mode='lines', name=label, legendgroup=label,
                      hovertemplate='Recall: %{x:.3f}<br>Precision: %{y:.3f}<br>Threshold: %{customdata:.3f}',
                      customdata=df_config['threshold']),
            row=1, col=1
        )
        
        # F0.5 vs threshold
        fig_html.add_trace(
            go.Scatter(x=df_config['threshold'], y=df_config['f05'],
                      mode='lines', name=label, legendgroup=label, showlegend=False,
                      hovertemplate='Threshold: %{x:.3f}<br>F0.5: %{y:.3f}'),
            row=1, col=2
        )
    
    # Add baseline
    fig_html.add_trace(
        go.Scatter(x=[0, 1], y=[BASELINE_PRECISION, BASELINE_PRECISION],
                  mode='lines', line=dict(dash='dash', color='red', width=2),
                  name=f'No-skill (P={BASELINE_PRECISION:.1%})'),
        row=1, col=1
    )
    
    fig_html.update_xaxes(title_text="Recall", range=[0, 1], row=1, col=1)
    fig_html.update_yaxes(title_text="Precision", range=[0, 1], row=1, col=1)
    fig_html.update_xaxes(title_text="Threshold", row=1, col=2)
    fig_html.update_yaxes(title_text="F0.5 Score", row=1, col=2)
    fig_html.update_layout(
        title="PR Curves - Selected Configurations",
        height=550, width=1300,
        legend=dict(font=dict(size=9))
    )
    _save_html(fig_html, "pr_curves_selected.html")
    print("  Saved: reports/pr_curves_selected.html")


def plot_roc_curves_selected():
    """
    Plot ROC curves from Phase 3 threshold tuning for selected configurations.
    Computes AUC using trapezoidal rule and displays in legend.
    """
    print("Plotting ROC curves for selected configurations...")
    
    try:
        df = pd.read_csv('roc_curve.csv')
    except FileNotFoundError:
        print("  Warning: roc_curve.csv not found")
        return
    
    # Get unique configurations
    config_cols = ['classifier', 'ngram', 'log_buckets', 'num_hashes', 'optimized_for']
    configs = df.groupby(config_cols).size().reset_index()[config_cols]
    
    print(f"  Found {len(configs)} configurations")
    
    # Color palettes
    fh_configs = configs[configs['classifier'] == 'FeatureHashing']
    cm_configs = configs[configs['classifier'] == 'CountMin']
    fh_colors = plt.cm.Blues(np.linspace(0.4, 0.9, max(len(fh_configs), 1)))
    cm_colors = plt.cm.Greens(np.linspace(0.4, 0.9, max(len(cm_configs), 1)))
    
    # === Matplotlib: ROC curve ===
    fig, ax = plt.subplots(figsize=(10, 9))
    
    fh_idx, cm_idx = 0, 0
    auc_data = []
    
    for _, config in configs.iterrows():
        mask = True
        for col in config_cols:
            mask = mask & (df[col] == config[col])
        df_config = df[mask].copy().sort_values('fpr')
        
        if len(df_config) == 0:
            continue
        
        # Calculate AUC using trapezoidal rule
        auc = np.trapz(df_config['tpr'], df_config['fpr'])
        
        base_label = _config_label(config)
        label = f"{base_label} (AUC={auc:.4f})"
        
        if config['classifier'] == 'FeatureHashing':
            color = fh_colors[fh_idx % len(fh_colors)]
            fh_idx += 1
        else:
            color = cm_colors[cm_idx % len(cm_colors)]
            cm_idx += 1
        
        ax.plot(df_config['fpr'], df_config['tpr'],
               linewidth=1.5, label=label, alpha=0.8, color=color)
        
        auc_data.append({
            'classifier': config['classifier'],
            'ngram': int(config['ngram']),
            'log_buckets': int(config['log_buckets']),
            'num_hashes': int(config['num_hashes']),
            'optimized_for': config['optimized_for'],
            'auc': auc
        })
    
    # Add diagonal (random classifier)
    ax.plot([0, 1], [0, 1], 'k--', linewidth=1, alpha=0.5, label='Random (AUC=0.5)')
    
    ax.set_xlabel('False Positive Rate (FPR)')
    ax.set_ylabel('True Positive Rate (TPR)')
    ax.set_title('ROC Curves - Selected Configurations')
    ax.legend(loc='lower right', fontsize=8)
    ax.grid(True, alpha=0.3)
    ax.set_xlim([0, 1])
    ax.set_ylim([0, 1])
    
    plt.tight_layout()
    _save_png(fig, 'roc_curves_selected.png')
    print("  Saved: reports/roc_curves_selected.png")
    
    # Save AUC data
    auc_df = pd.DataFrame(auc_data).sort_values('auc', ascending=False)
    auc_df.to_csv(OUTPUT_DIR / 'auc_scores_selected.csv', index=False)
    print("  Saved: reports/auc_scores_selected.csv")
    
    # === Plotly: Interactive ROC ===
    fig_html = go.Figure()
    
    for _, config in configs.iterrows():
        mask = True
        for col in config_cols:
            mask = mask & (df[col] == config[col])
        df_config = df[mask].copy().sort_values('fpr')
        
        if len(df_config) == 0:
            continue
        
        auc = np.trapz(df_config['tpr'], df_config['fpr'])
        base_label = _config_label(config)
        label = f"{base_label} (AUC={auc:.4f})"
        
        fig_html.add_trace(
            go.Scatter(x=df_config['fpr'], y=df_config['tpr'],
                      mode='lines', name=label,
                      hovertemplate='FPR: %{x:.3f}<br>TPR: %{y:.3f}<br>Threshold: %{customdata:.3f}',
                      customdata=df_config['threshold'])
        )
    
    # Add diagonal
    fig_html.add_trace(
        go.Scatter(x=[0, 1], y=[0, 1], mode='lines',
                  line=dict(dash='dash', color='black'),
                  name='Random (AUC=0.5)')
    )
    
    fig_html.update_xaxes(title_text="False Positive Rate", range=[0, 1])
    fig_html.update_yaxes(title_text="True Positive Rate", range=[0, 1])
    fig_html.update_layout(
        title="ROC Curves - Selected Configurations",
        height=700, width=900,
        legend=dict(font=dict(size=9))
    )
    _save_html(fig_html, "roc_curves_selected.html")
    print("  Saved: reports/roc_curves_selected.html")


def plot_learning_curves_selected():
    """
    Plot learning curves for selected configurations.
    Shows precision, recall, accuracy, f1, f0.5 over training examples.
    """
    print("Plotting learning curves for selected configurations...")
    
    try:
        df = pd.read_csv('learning_curves.csv')
    except FileNotFoundError:
        print("  Warning: learning_curves.csv not found")
        return
    
    print(f"  Loaded {len(df)} data points")
    
    metrics = ['precision', 'recall', 'accuracy', 'f1', 'f05']
    metric_labels = ['Precision', 'Recall', 'Accuracy', 'F1', 'F0.5']
    
    # Get unique configurations
    config_cols = ['classifier', 'ngram', 'log_buckets', 'num_hashes', 'optimized_for', 'threshold']
    configs = df.groupby(config_cols).size().reset_index()[config_cols]
    
    print(f"  Found {len(configs)} unique configurations")
    
    # Get unique window sizes
    window_sizes = sorted(df['window'].unique())
    
    # === Matplotlib: Grid of metric subplots ===
    fig, axes = plt.subplots(2, 3, figsize=(18, 10))
    axes = axes.flatten()
    
    # Create color map for configs
    colors = plt.cm.tab10(np.linspace(0, 1, len(configs)))
    linestyles = ['-', '--', ':', '-.']
    
    for idx, (metric, label) in enumerate(zip(metrics, metric_labels)):
        ax = axes[idx]
        
        for cfg_idx, (_, config) in enumerate(configs.iterrows()):
            config_label = _config_label(config)
            color = colors[cfg_idx % len(colors)]
            
            for ws_idx, ws in enumerate(window_sizes):
                mask = (df['classifier'] == config['classifier']) & \
                       (df['ngram'] == config['ngram']) & \
                       (df['log_buckets'] == config['log_buckets']) & \
                       (df['num_hashes'] == config['num_hashes']) & \
                       (df['optimized_for'] == config['optimized_for']) & \
                       (df['window'] == ws)
                
                subset = df[mask].sort_values('num_examples')
                
                if len(subset) == 0:
                    continue
                
                # Only show legend for first window size
                show_label = f"{config_label} w={ws}" if ws == window_sizes[0] else None
                linestyle = linestyles[ws_idx % len(linestyles)]
                
                ax.plot(subset['num_examples'], subset[metric],
                       linestyle=linestyle, linewidth=1.5, alpha=0.7,
                       color=color, label=show_label if ws == window_sizes[0] else None)
        
        ax.set_xlabel('Number of Training Examples')
        ax.set_ylabel(label)
        ax.set_title(f'{label} Learning Curve')
        ax.grid(True, alpha=0.3)
        if idx == 0:
            ax.legend(loc='best', fontsize=7)
    
    # Hide unused subplot
    axes[-1].set_visible(False)
    
    plt.tight_layout()
    _save_png(fig, 'learning_curves_selected.png')
    print("  Saved: reports/learning_curves_selected.png")
    
    # === Matplotlib: Individual metric plots (all windows) ===
    for metric, label in zip(metrics, metric_labels):
        fig, ax = plt.subplots(figsize=(12, 7))
        
        for cfg_idx, (_, config) in enumerate(configs.iterrows()):
            config_label = _config_label(config)
            color = colors[cfg_idx % len(colors)]
            
            for ws_idx, ws in enumerate(window_sizes):
                mask = (df['classifier'] == config['classifier']) & \
                       (df['ngram'] == config['ngram']) & \
                       (df['log_buckets'] == config['log_buckets']) & \
                       (df['num_hashes'] == config['num_hashes']) & \
                       (df['optimized_for'] == config['optimized_for']) & \
                       (df['window'] == ws)
                
                subset = df[mask].sort_values('num_examples')
                
                if len(subset) == 0:
                    continue
                
                linestyle = linestyles[ws_idx % len(linestyles)]
                full_label = f"{config_label} w={ws}"
                
                ax.plot(subset['num_examples'], subset[metric],
                       linestyle=linestyle, linewidth=1.5, alpha=0.7,
                       color=color, label=full_label)
        
        ax.set_xlabel('Number of Training Examples')
        ax.set_ylabel(label)
        ax.set_title(f'{label} Learning Curves - All Configurations')
        ax.legend(loc='best', fontsize=7, ncol=2)
        ax.grid(True, alpha=0.3)
        
        plt.tight_layout()
        _save_png(fig, f'learning_curve_{metric}.png')
        print(f"  Saved: reports/learning_curve_{metric}.png")
    
    # === Plotly: Interactive with dropdown for metric selection ===
    fig_html = go.Figure()
    
    # Add traces for all metrics (will toggle visibility)
    for metric_idx, (metric, label) in enumerate(zip(metrics, metric_labels)):
        for cfg_idx, (_, config) in enumerate(configs.iterrows()):
            config_label = _config_label(config)
            
            for ws in window_sizes:
                mask = (df['classifier'] == config['classifier']) & \
                       (df['ngram'] == config['ngram']) & \
                       (df['log_buckets'] == config['log_buckets']) & \
                       (df['num_hashes'] == config['num_hashes']) & \
                       (df['optimized_for'] == config['optimized_for']) & \
                       (df['window'] == ws)
                
                subset = df[mask].sort_values('num_examples')
                
                if len(subset) == 0:
                    continue
                
                fig_html.add_trace(
                    go.Scatter(
                        x=subset['num_examples'],
                        y=subset[metric],
                        mode='lines+markers',
                        name=f"{config_label} w={ws}",
                        visible=(metric_idx == 4),  # F0.5 visible by default
                        legendgroup=f"{config_label} w={ws}",
                        showlegend=(metric_idx == 4),
                        hovertemplate=f'{label}: %{{y:.3f}}<br>Examples: %{{x}}'
                    )
                )
    
    # Calculate traces per metric
    traces_per_metric = len(configs) * len(window_sizes)
    
    # Create dropdown buttons
    buttons = []
    for metric_idx, (metric, label) in enumerate(zip(metrics, metric_labels)):
        visibility = [False] * (len(metrics) * traces_per_metric)
        for i in range(traces_per_metric):
            visibility[metric_idx * traces_per_metric + i] = True
        
        buttons.append(dict(
            label=label,
            method='update',
            args=[{'visible': visibility},
                  {'title': f'{label} Learning Curves - Selected Configurations'}]
        ))
    
    fig_html.update_layout(
        updatemenus=[dict(
            active=4,  # F0.5 default
            buttons=buttons,
            direction="down",
            x=0.1,
            y=1.15,
            showactive=True,
        )],
        title="F0.5 Learning Curves - Selected Configurations",
        xaxis_title="Number of Training Examples",
        yaxis_title="Score",
        height=600,
        width=1000
    )
    _save_html(fig_html, "learning_curves_selected.html")
    print("  Saved: reports/learning_curves_selected.html")


def plot_timing_4d():
    """
    Create 4D timing visualization: ngram (x), log_buckets (y), num_hashes (z), time (color).
    Uses 3D scatter plot with color representing time per email.
    """
    print("Plotting 4D timing visualization...")
    
    try:
        df = pd.read_csv('timing_results.csv')
    except FileNotFoundError:
        print("  Warning: timing_results.csv not found")
        return
    
    print(f"  Loaded {len(df)} timing data points")
    
    # Separate Feature Hashing (num_hashes=0) and Count-Min
    df_fh = df[df['num_hashes'] == 0].copy()
    df_cm = df[df['num_hashes'] > 0].copy()
    
    # === Plotly: Interactive 3D scatter ===
    fig_3d = go.Figure()
    
    # Feature Hashing (show as z=0 plane since no num_hashes)
    if len(df_fh) > 0:
        fig_3d.add_trace(
            go.Scatter3d(
                x=df_fh['ngram'],
                y=df_fh['log_buckets'],
                z=[0] * len(df_fh),  # z=0 for Feature Hashing
                mode='markers',
                marker=dict(
                    size=10,
                    color=df_fh['time_per_email_ms'],
                    colorscale='Blues',
                    colorbar=dict(title='Time (ms)', x=0.85),
                    showscale=True,
                ),
                name='Feature Hashing',
                text=[f"FH: ng={ng}, lb={lb}<br>Time: {t:.3f}ms" 
                      for ng, lb, t in zip(df_fh['ngram'], df_fh['log_buckets'], df_fh['time_per_email_ms'])],
                hoverinfo='text'
            )
        )
    
    # Count-Min
    if len(df_cm) > 0:
        fig_3d.add_trace(
            go.Scatter3d(
                x=df_cm['ngram'],
                y=df_cm['log_buckets'],
                z=df_cm['num_hashes'],
                mode='markers',
                marker=dict(
                    size=8,
                    color=df_cm['time_per_email_ms'],
                    colorscale='Greens',
                    colorbar=dict(title='Time (ms)', x=1.0),
                    showscale=True,
                ),
                name='Count-Min',
                text=[f"CM: ng={ng}, lb={lb}, nh={nh}<br>Time: {t:.3f}ms" 
                      for ng, lb, nh, t in zip(df_cm['ngram'], df_cm['log_buckets'], 
                                               df_cm['num_hashes'], df_cm['time_per_email_ms'])],
                hoverinfo='text'
            )
        )
    
    fig_3d.update_layout(
        title="Computational Efficiency: 4D Visualization<br>(ngram x log_buckets x num_hashes, color=time)",
        scene=dict(
            xaxis_title='N-gram Size',
            yaxis_title='Log Num Buckets',
            zaxis_title='Num Hashes (0=FH)',
        ),
        height=700,
        width=900,
    )
    _save_html(fig_3d, "timing_4d.html")
    print("  Saved: reports/timing_4d.html")
    
    # === Matplotlib: 2D heatmaps for each classifier ===
    fig, axes = plt.subplots(1, 2, figsize=(14, 5))
    
    # Feature Hashing heatmap
    ax = axes[0]
    if len(df_fh) > 0:
        pivot_fh = df_fh.pivot_table(values='time_per_email_ms', 
                                      index='ngram', columns='log_buckets')
        sns.heatmap(pivot_fh, annot=True, fmt='.3f', cmap='Blues', ax=ax,
                   cbar_kws={'label': 'Time per email (ms)'})
        ax.set_title('Feature Hashing: Time per Email')
        ax.set_xlabel('Log Num Buckets')
        ax.set_ylabel('N-gram Size')
    else:
        ax.text(0.5, 0.5, 'No Feature Hashing data', ha='center', va='center')
        ax.set_title('Feature Hashing')
    
    # Count-Min: average across num_hashes
    ax = axes[1]
    if len(df_cm) > 0:
        pivot_cm = df_cm.pivot_table(values='time_per_email_ms', 
                                      index='ngram', columns='log_buckets', aggfunc='mean')
        sns.heatmap(pivot_cm, annot=True, fmt='.3f', cmap='Greens', ax=ax,
                   cbar_kws={'label': 'Time per email (ms)'})
        ax.set_title('Count-Min: Avg Time per Email (across num_hashes)')
        ax.set_xlabel('Log Num Buckets')
        ax.set_ylabel('N-gram Size')
    else:
        ax.text(0.5, 0.5, 'No Count-Min data', ha='center', va='center')
        ax.set_title('Count-Min')
    
    plt.tight_layout()
    _save_png(fig, 'timing_heatmaps.png')
    print("  Saved: reports/timing_heatmaps.png")
    
    # === Bar chart comparison ===
    fig, ax = plt.subplots(figsize=(14, 6))
    
    # Create labels
    labels = []
    times = []
    colors = []
    
    for _, row in df.iterrows():
        if row['num_hashes'] == 0:
            labels.append(f"FH ng={int(row['ngram'])} lb={int(row['log_buckets'])}")
            colors.append('steelblue')
        else:
            labels.append(f"CM ng={int(row['ngram'])} lb={int(row['log_buckets'])} nh={int(row['num_hashes'])}")
            colors.append('forestgreen')
        times.append(row['time_per_email_ms'])
    
    x = np.arange(len(labels))
    ax.bar(x, times, color=colors, alpha=0.8)
    ax.set_xticks(x)
    ax.set_xticklabels(labels, rotation=90, fontsize=7)
    ax.set_ylabel('Time per Email (ms)')
    ax.set_title('Computational Efficiency: Time per Email by Configuration')
    ax.grid(True, alpha=0.3, axis='y')
    
    # Add legend
    from matplotlib.patches import Patch
    legend_elements = [Patch(facecolor='steelblue', alpha=0.8, label='Feature Hashing'),
                      Patch(facecolor='forestgreen', alpha=0.8, label='Count-Min')]
    ax.legend(handles=legend_elements)
    
    plt.tight_layout()
    _save_png(fig, 'timing_bar.png')
    print("  Saved: reports/timing_bar.png")


def plot_test_vs_dev():
    """
    Plot comparison of test vs dev performance for final configurations.
    Highlights potential overfitting when test << dev.
    """
    print("Plotting test vs dev comparison...")
    
    try:
        df = pd.read_csv('final_test_results.csv')
    except FileNotFoundError:
        print("  Warning: evalution_results.csv not found")
        return
    
    print(f"  Loaded {len(df)} final configurations")
    
    # Metrics to compare
    metric_pairs = [
        ('test_f05', 'dev_f05', 'F0.5'),
        ('test_precision', 'dev_precision', 'Precision'),
        ('test_recall', 'dev_recall', 'Recall'),
        ('test_accuracy', 'dev_accuracy', 'Accuracy'),
    ]
    
    # === Matplotlib: Grouped bar chart ===
    fig, axes = plt.subplots(2, 2, figsize=(14, 10))
    axes = axes.flatten()
    
    # Create configuration labels
    labels = [_config_label(row) for _, row in df.iterrows()]
    x = np.arange(len(labels))
    width = 0.35
    
    for idx, (test_col, dev_col, label) in enumerate(metric_pairs):
        ax = axes[idx]
        
        dev_vals = df[dev_col].values
        test_vals = df[test_col].values
        
        bars_dev = ax.bar(x - width/2, dev_vals, width, label='Dev Set', 
                         color='steelblue', alpha=0.8)
        bars_test = ax.bar(x + width/2, test_vals, width, label='Test Set', 
                          color='coral', alpha=0.8)
        
        # Highlight significant drops (overfitting)
        for i, (dev_v, test_v) in enumerate(zip(dev_vals, test_vals)):
            if test_v < dev_v - 0.03:  # More than 3% drop
                ax.annotate('!', xy=(x[i] + width/2, test_v), 
                           ha='center', va='bottom', fontsize=14, color='red', fontweight='bold')
        
        ax.set_xlabel('Configuration')
        ax.set_ylabel(label)
        ax.set_title(f'{label}: Dev vs Test Comparison')
        ax.set_xticks(x)
        ax.set_xticklabels(labels, rotation=45, ha='right', fontsize=7)
        ax.legend()
        ax.grid(True, alpha=0.3, axis='y')
    
    plt.tight_layout()
    _save_png(fig, 'test_vs_dev.png')
    print("  Saved: reports/test_vs_dev.png")
    
    # === Matplotlib: Scatter plot dev vs test ===
    fig, ax = plt.subplots(figsize=(10, 8))
    
    # Plot each metric with different markers
    markers = ['o', 's', '^', 'D']
    colors_m = ['blue', 'green', 'red', 'purple']
    
    for idx, (test_col, dev_col, label) in enumerate(metric_pairs):
        ax.scatter(df[dev_col], df[test_col], 
                  marker=markers[idx], s=100, alpha=0.7, 
                  color=colors_m[idx], label=label)
    
    # Add diagonal (perfect agreement)
    ax.plot([0.5, 1], [0.5, 1], 'k--', linewidth=1, alpha=0.5, label='Perfect agreement')
    
    ax.set_xlabel('Dev Set Score')
    ax.set_ylabel('Test Set Score')
    ax.set_title('Dev vs Test Performance (points below diagonal = overfitting)')
    ax.legend(loc='lower right')
    ax.grid(True, alpha=0.3)
    ax.set_xlim([0.5, 1.0])
    ax.set_ylim([0.5, 1.0])
    
    plt.tight_layout()
    _save_png(fig, 'test_vs_dev_scatter.png')
    print("  Saved: reports/test_vs_dev_scatter.png")
    
    # === Plotly: Interactive grouped bar ===
    fig_html = go.Figure()
    
    for test_col, dev_col, label in metric_pairs:
        fig_html.add_trace(go.Bar(
            name=f'{label} (Dev)',
            x=labels,
            y=df[dev_col],
            marker_color='steelblue',
            opacity=0.8
        ))
        fig_html.add_trace(go.Bar(
            name=f'{label} (Test)',
            x=labels,
            y=df[test_col],
            marker_color='coral',
            opacity=0.8
        ))
    
    fig_html.update_layout(
        barmode='group',
        title="Test vs Dev Comparison - All Metrics",
        xaxis_title="Configuration",
        yaxis_title="Score",
        height=600,
        width=1200,
        legend=dict(font=dict(size=9))
    )
    _save_html(fig_html, "test_vs_dev.html")
    print("  Saved: reports/test_vs_dev.html")
    
    # Print summary with warnings
    print("\n  === Test vs Dev Summary ===")
    for _, row in df.iterrows():
        config_label = _config_label(row)
        f05_diff = row['test_f05'] - row['dev_f05']
        
        print(f"  {config_label}:")
        print(f"    Dev:  F0.5={row['dev_f05']:.4f}, P={row['dev_precision']:.4f}, R={row['dev_recall']:.4f}")
        print(f"    Test: F0.5={row['test_f05']:.4f}, P={row['test_precision']:.4f}, R={row['test_recall']:.4f}")
        
        if f05_diff < -0.03:
            print(f"    WARNING: Test F0.5 is {-f05_diff*100:.1f}% lower - possible overfitting!")


def plot_config_summary():
    """
    Create summary visualization of final configurations.
    Shows F0.5 scores and parameter comparison.
    """
    print("Plotting configuration summary...")
    
    try:
        df = pd.read_csv('final_configs.csv')
    except FileNotFoundError:
        print("  Warning: final_configs.csv not found")
        return
    
    print(f"  Loaded {len(df)} final configurations")
    
    # === Matplotlib: Bar chart of F0.5 scores ===
    fig, ax = plt.subplots(figsize=(12, 6))
    
    labels = [_config_label(row, include_opt=True) for _, row in df.iterrows()]
    colors = ['steelblue' if row['classifier'] == 'FeatureHashing' else 'forestgreen' 
              for _, row in df.iterrows()]
    
    x = np.arange(len(labels))
    bars = ax.bar(x, df['f05'], color=colors, alpha=0.8)
    
    # Add value labels on bars
    for bar, val in zip(bars, df['f05']):
        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.005,
               f'{val:.3f}', ha='center', va='bottom', fontsize=9)
    
    ax.set_xticks(x)
    ax.set_xticklabels(labels, rotation=45, ha='right', fontsize=9)
    ax.set_ylabel('F0.5 Score')
    ax.set_title('Best F0.5 Scores by Configuration')
    ax.grid(True, alpha=0.3, axis='y')
    
    # Legend
    from matplotlib.patches import Patch
    legend_elements = [Patch(facecolor='steelblue', alpha=0.8, label='Feature Hashing'),
                      Patch(facecolor='forestgreen', alpha=0.8, label='Count-Min')]
    ax.legend(handles=legend_elements)
    
    plt.tight_layout()
    _save_png(fig, 'config_summary_f05.png')
    print("  Saved: reports/config_summary_f05.png")
    
    # === Matplotlib: Multi-metric comparison ===
    fig, ax = plt.subplots(figsize=(14, 7))
    
    metrics = ['f05', 'precision', 'recall', 'f1', 'accuracy']
    x = np.arange(len(labels))
    width = 0.15
    
    colors_metrics = plt.cm.Set2(np.linspace(0, 1, len(metrics)))
    
    for i, metric in enumerate(metrics):
        offset = (i - len(metrics)/2 + 0.5) * width
        ax.bar(x + offset, df[metric], width, label=metric.upper(), 
               color=colors_metrics[i], alpha=0.9)
    
    ax.set_xticks(x)
    ax.set_xticklabels(labels, rotation=45, ha='right', fontsize=8)
    ax.set_ylabel('Score')
    ax.set_title('All Metrics by Configuration')
    ax.legend(loc='upper right')
    ax.grid(True, alpha=0.3, axis='y')
    
    plt.tight_layout()
    _save_png(fig, 'config_summary_all_metrics.png')
    print("  Saved: reports/config_summary_all_metrics.png")
    
    # === Plotly: Interactive radar chart ===
    fig_radar = go.Figure()
    
    categories = ['F0.5', 'Precision', 'Recall', 'F1', 'Accuracy']
    
    for _, row in df.iterrows():
        label = _config_label(row)
        values = [row['f05'], row['precision'], row['recall'], row['f1'], row['accuracy']]
        values.append(values[0])  # Close the polygon
        
        fig_radar.add_trace(go.Scatterpolar(
            r=values,
            theta=categories + [categories[0]],
            fill='toself',
            name=label,
            opacity=0.6
        ))
    
    fig_radar.update_layout(
        polar=dict(radialaxis=dict(visible=True, range=[0.5, 1.0])),
        title="Configuration Comparison - Radar Chart",
        height=600,
        width=800
    )
    _save_html(fig_radar, "config_radar.html")
    print("  Saved: reports/config_radar.html")
    
    # === Summary table ===
    print("\n  === Configuration Summary ===")
    print(f"  {'Configuration':<45} {'F0.5':>8} {'Prec':>8} {'Recall':>8} {'Thresh':>8}")
    print("  " + "-" * 80)
    
    for _, row in df.iterrows():
        label = _config_label(row)
        print(f"  {label:<45} {row['f05']:>8.4f} {row['precision']:>8.4f} "
              f"{row['recall']:>8.4f} {row['best_threshold']:>8.3f}")
    
    # Find best overall
    best_idx = df['f05'].idxmax()
    best = df.loc[best_idx]
    print(f"\n  Best configuration: {_config_label(best)} with F0.5={best['f05']:.4f}")


def plot_cv_recall_at_precision():
    """
    Plot Recall@Precision=0.95 from cross-validation results.
    - Feature Hashing: 2D heatmap (ngram x log_buckets)
    - Count-Min: 3D visualization (ngram x log_buckets x num_hashes)
    """
    print("Plotting CV Recall@Precision=0.95...")
    
    try:
        df = pd.read_csv('cv_results.csv')
    except FileNotFoundError:
        print("  Warning: cv_results.csv not found")
        return
    
    print(f"  Loaded {len(df)} CV data points")
    
    # Average across folds
    agg_cols = {
        'recall_at_prec_95': 'mean',
        'recall_at_prec_99': 'mean',
        'auc': 'mean',
        'recall_at_fpr_001': 'mean',
        'recall_at_fpr_0001': 'mean'
    }
    # Add brier_score if present in data
    if 'brier_score' in df.columns:
        agg_cols['brier_score'] = 'mean'
    agg_df = df.groupby(['classifier', 'ngram', 'log_buckets', 'num_hashes']).agg(agg_cols).reset_index()
    
    # === Feature Hashing: 2D Heatmap ===
    df_fh = agg_df[agg_df['classifier'] == 'FeatureHashing']
    
    if len(df_fh) > 0:
        fig, ax = plt.subplots(figsize=(10, 7))
        
        pivot_fh = df_fh.pivot_table(values='recall_at_prec_95', 
                                      index='ngram', columns='log_buckets')
        
        sns.heatmap(pivot_fh, annot=True, fmt='.3f', cmap='YlOrRd', ax=ax,
                   cbar_kws={'label': 'Recall @ Precision=0.95'},
                   vmin=0, vmax=1)
        ax.set_title('Feature Hashing: Recall @ Precision=0.95 (CV Average)')
        ax.set_xlabel('Log Num Buckets')
        ax.set_ylabel('N-gram Size')
        
        plt.tight_layout()
        _save_png(fig, 'cv_recall_at_prec95_fh.png')
        print("  Saved: reports/cv_recall_at_prec95_fh.png")
    
    # === Count-Min: 3D Visualization ===
    df_cm = agg_df[agg_df['classifier'] == 'CountMin']
    
    if len(df_cm) > 0:
        # 3D scatter plot with color = recall_at_prec_95
        fig_3d = go.Figure()
        
        fig_3d.add_trace(
            go.Scatter3d(
                x=df_cm['ngram'],
                y=df_cm['log_buckets'],
                z=df_cm['num_hashes'],
                mode='markers',
                marker=dict(
                    size=10,
                    color=df_cm['recall_at_prec_95'],
                    colorscale='YlOrRd',
                    colorbar=dict(title='Recall@P=0.95'),
                    showscale=True,
                    cmin=0,
                    cmax=1,
                ),
                text=[f"ng={ng}, lb={lb}, nh={nh}<br>Rec@P95: {r:.3f}" 
                      for ng, lb, nh, r in zip(df_cm['ngram'], df_cm['log_buckets'], 
                                               df_cm['num_hashes'], df_cm['recall_at_prec_95'])],
                hoverinfo='text'
            )
        )
        
        fig_3d.update_layout(
            title="Count-Min: Recall @ Precision=0.95 (CV Average)",
            scene=dict(
                xaxis_title='N-gram Size',
                yaxis_title='Log Num Buckets',
                zaxis_title='Num Hashes',
            ),
            height=700,
            width=900,
        )
        _save_html(fig_3d, "cv_recall_at_prec95_cm_3d.html")
        print("  Saved: reports/cv_recall_at_prec95_cm_3d.html")
        
        # Also create 2D heatmaps for each num_hashes value
        num_hashes_vals = sorted(df_cm['num_hashes'].unique())
        
        fig, axes = plt.subplots(1, len(num_hashes_vals), figsize=(5*len(num_hashes_vals), 5))
        if len(num_hashes_vals) == 1:
            axes = [axes]
        
        for i, nh in enumerate(num_hashes_vals):
            ax = axes[i]
            df_subset = df_cm[df_cm['num_hashes'] == nh]
            pivot = df_subset.pivot_table(values='recall_at_prec_95', 
                                          index='ngram', columns='log_buckets')
            
            sns.heatmap(pivot, annot=True, fmt='.3f', cmap='YlOrRd', ax=ax,
                       cbar_kws={'label': 'Recall@P=0.95'},
                       vmin=0, vmax=1)
            ax.set_title(f'Count-Min (nh={int(nh)})')
            ax.set_xlabel('Log Num Buckets')
            ax.set_ylabel('N-gram Size')
        
        plt.tight_layout()
        _save_png(fig, 'cv_recall_at_prec95_cm.png')
        print("  Saved: reports/cv_recall_at_prec95_cm.png")
    
    # === Combined interactive heatmap ===
    fig_html = make_subplots(
        rows=1, cols=2,
        subplot_titles=["Feature Hashing", "Count-Min (avg across num_hashes)"],
        horizontal_spacing=0.12
    )
    
    if len(df_fh) > 0:
        pivot_fh = df_fh.pivot_table(values='recall_at_prec_95', 
                                      index='ngram', columns='log_buckets')
        fig_html.add_trace(
            go.Heatmap(
                z=pivot_fh.values,
                x=pivot_fh.columns.astype(str),
                y=pivot_fh.index.astype(str),
                colorscale='YlOrRd',
                zmin=0, zmax=1,
                colorbar=dict(title='Rec@P95', x=0.45),
                showscale=True,
                text=[[f'{v:.3f}' for v in row] for row in pivot_fh.values],
                texttemplate='%{text}',
                hovertemplate='ngram=%{y}<br>log_buckets=%{x}<br>Rec@P95=%{z:.3f}<extra></extra>'
            ),
            row=1, col=1
        )
    
    if len(df_cm) > 0:
        pivot_cm = df_cm.pivot_table(values='recall_at_prec_95', 
                                      index='ngram', columns='log_buckets', aggfunc='mean')
        fig_html.add_trace(
            go.Heatmap(
                z=pivot_cm.values,
                x=pivot_cm.columns.astype(str),
                y=pivot_cm.index.astype(str),
                colorscale='YlOrRd',
                zmin=0, zmax=1,
                colorbar=dict(title='Rec@P95', x=1.0),
                showscale=True,
                text=[[f'{v:.3f}' for v in row] for row in pivot_cm.values],
                texttemplate='%{text}',
                hovertemplate='ngram=%{y}<br>log_buckets=%{x}<br>Rec@P95=%{z:.3f}<extra></extra>'
            ),
            row=1, col=2
        )
    
    fig_html.update_xaxes(title_text="Log Num Buckets", row=1, col=1)
    fig_html.update_yaxes(title_text="N-gram Size", row=1, col=1)
    fig_html.update_xaxes(title_text="Log Num Buckets", row=1, col=2)
    fig_html.update_yaxes(title_text="N-gram Size", row=1, col=2)
    fig_html.update_layout(
        title="CV Recall @ Precision=0.95",
        height=500,
        width=1000
    )
    _save_html(fig_html, "cv_recall_at_prec95.html")
    print("  Saved: reports/cv_recall_at_prec95.html")


def plot_timing_by_dimension():
    """
    Plot timing by each hyperparameter dimension.
    Shows average time vs ngram, log_buckets, and num_hashes separately.
    """
    print("Plotting timing by dimension...")
    
    try:
        df = pd.read_csv('timing_results.csv')
    except FileNotFoundError:
        print("  Warning: timing_results.csv not found")
        return
    
    print(f"  Loaded {len(df)} timing data points")
    
    # Separate classifiers
    df_fh = df[df['num_hashes'] == 0].copy()
    df_cm = df[df['num_hashes'] > 0].copy()
    
    # === Matplotlib: 3 subplots for each dimension ===
    fig, axes = plt.subplots(1, 3, figsize=(16, 5))
    
    # --- Plot 1: Time vs N-gram ---
    ax = axes[0]
    
    # Feature Hashing: average across log_buckets
    if len(df_fh) > 0:
        fh_by_ngram = df_fh.groupby('ngram')['time_per_email_ms'].mean()
        ax.plot(fh_by_ngram.index, fh_by_ngram.values, 'o-', 
               linewidth=2, markersize=8, color='steelblue', label='Feature Hashing')
    
    # Count-Min: average across log_buckets and num_hashes
    if len(df_cm) > 0:
        cm_by_ngram = df_cm.groupby('ngram')['time_per_email_ms'].mean()
        ax.plot(cm_by_ngram.index, cm_by_ngram.values, 's--', 
               linewidth=2, markersize=8, color='forestgreen', label='Count-Min')
    
    ax.set_xlabel('N-gram Size')
    ax.set_ylabel('Avg Time per Email (ms)')
    ax.set_title('Time vs N-gram Size')
    ax.legend()
    ax.grid(True, alpha=0.3)
    ax.set_xticks(sorted(df['ngram'].unique()))
    
    # --- Plot 2: Time vs Log Buckets ---
    ax = axes[1]
    
    if len(df_fh) > 0:
        fh_by_lb = df_fh.groupby('log_buckets')['time_per_email_ms'].mean()
        ax.plot(fh_by_lb.index, fh_by_lb.values, 'o-', 
               linewidth=2, markersize=8, color='steelblue', label='Feature Hashing')
    
    if len(df_cm) > 0:
        cm_by_lb = df_cm.groupby('log_buckets')['time_per_email_ms'].mean()
        ax.plot(cm_by_lb.index, cm_by_lb.values, 's--', 
               linewidth=2, markersize=8, color='forestgreen', label='Count-Min')
    
    ax.set_xlabel('Log Num Buckets')
    ax.set_ylabel('Avg Time per Email (ms)')
    ax.set_title('Time vs Log Num Buckets')
    ax.legend()
    ax.grid(True, alpha=0.3)
    ax.set_xticks(sorted(df['log_buckets'].unique()))
    
    # --- Plot 3: Time vs Num Hashes (Count-Min only) ---
    ax = axes[2]
    
    if len(df_cm) > 0:
        cm_by_nh = df_cm.groupby('num_hashes')['time_per_email_ms'].mean()
        ax.plot(cm_by_nh.index, cm_by_nh.values, 's-', 
               linewidth=2, markersize=8, color='forestgreen', label='Count-Min')
        ax.set_xticks(sorted(df_cm['num_hashes'].unique()))
    else:
        ax.text(0.5, 0.5, 'No Count-Min data', ha='center', va='center', transform=ax.transAxes)
    
    ax.set_xlabel('Num Hashes')
    ax.set_ylabel('Avg Time per Email (ms)')
    ax.set_title('Time vs Num Hashes (Count-Min only)')
    if len(df_cm) > 0:
        ax.legend()
    ax.grid(True, alpha=0.3)
    
    plt.tight_layout()
    _save_png(fig, 'timing_by_dimension.png')
    print("  Saved: reports/timing_by_dimension.png")
    
    # === Bar chart version ===
    fig, axes = plt.subplots(1, 3, figsize=(16, 5))
    
    # N-gram bars
    ax = axes[0]
    ngrams = sorted(df['ngram'].unique())
    x = np.arange(len(ngrams))
    width = 0.35
    
    if len(df_fh) > 0:
        fh_vals = [df_fh[df_fh['ngram'] == ng]['time_per_email_ms'].mean() for ng in ngrams]
        ax.bar(x - width/2, fh_vals, width, label='Feature Hashing', color='steelblue', alpha=0.8)
    
    if len(df_cm) > 0:
        cm_vals = [df_cm[df_cm['ngram'] == ng]['time_per_email_ms'].mean() for ng in ngrams]
        ax.bar(x + width/2, cm_vals, width, label='Count-Min', color='forestgreen', alpha=0.8)
    
    ax.set_xlabel('N-gram Size')
    ax.set_ylabel('Avg Time per Email (ms)')
    ax.set_title('Time vs N-gram Size')
    ax.set_xticks(x)
    ax.set_xticklabels(ngrams)
    ax.legend()
    ax.grid(True, alpha=0.3, axis='y')
    
    # Log buckets bars
    ax = axes[1]
    log_buckets = sorted(df['log_buckets'].unique())
    x = np.arange(len(log_buckets))
    
    if len(df_fh) > 0:
        fh_vals = [df_fh[df_fh['log_buckets'] == lb]['time_per_email_ms'].mean() for lb in log_buckets]
        ax.bar(x - width/2, fh_vals, width, label='Feature Hashing', color='steelblue', alpha=0.8)
    
    if len(df_cm) > 0:
        cm_vals = [df_cm[df_cm['log_buckets'] == lb]['time_per_email_ms'].mean() for lb in log_buckets]
        ax.bar(x + width/2, cm_vals, width, label='Count-Min', color='forestgreen', alpha=0.8)
    
    ax.set_xlabel('Log Num Buckets')
    ax.set_ylabel('Avg Time per Email (ms)')
    ax.set_title('Time vs Log Num Buckets')
    ax.set_xticks(x)
    ax.set_xticklabels(log_buckets)
    ax.legend()
    ax.grid(True, alpha=0.3, axis='y')
    
    # Num hashes bars (Count-Min only)
    ax = axes[2]
    if len(df_cm) > 0:
        num_hashes = sorted(df_cm['num_hashes'].unique())
        x = np.arange(len(num_hashes))
        cm_vals = [df_cm[df_cm['num_hashes'] == nh]['time_per_email_ms'].mean() for nh in num_hashes]
        ax.bar(x, cm_vals, 0.6, label='Count-Min', color='forestgreen', alpha=0.8)
        ax.set_xticks(x)
        ax.set_xticklabels([int(nh) for nh in num_hashes])
        ax.legend()
    else:
        ax.text(0.5, 0.5, 'No Count-Min data', ha='center', va='center', transform=ax.transAxes)
    
    ax.set_xlabel('Num Hashes')
    ax.set_ylabel('Avg Time per Email (ms)')
    ax.set_title('Time vs Num Hashes (Count-Min only)')
    ax.grid(True, alpha=0.3, axis='y')
    
    plt.tight_layout()
    _save_png(fig, 'timing_by_dimension_bars.png')
    print("  Saved: reports/timing_by_dimension_bars.png")
    
    # === Plotly interactive ===
    fig_html = make_subplots(
        rows=1, cols=3,
        subplot_titles=["Time vs N-gram", "Time vs Log Buckets", "Time vs Num Hashes"],
        horizontal_spacing=0.08
    )
    
    # N-gram
    if len(df_fh) > 0:
        fh_by_ngram = df_fh.groupby('ngram')['time_per_email_ms'].mean().reset_index()
        fig_html.add_trace(
            go.Scatter(x=fh_by_ngram['ngram'], y=fh_by_ngram['time_per_email_ms'],
                      mode='lines+markers', name='Feature Hashing',
                      line=dict(color='steelblue'), marker=dict(size=10)),
            row=1, col=1
        )
    
    if len(df_cm) > 0:
        cm_by_ngram = df_cm.groupby('ngram')['time_per_email_ms'].mean().reset_index()
        fig_html.add_trace(
            go.Scatter(x=cm_by_ngram['ngram'], y=cm_by_ngram['time_per_email_ms'],
                      mode='lines+markers', name='Count-Min',
                      line=dict(color='forestgreen', dash='dash'), marker=dict(size=10, symbol='square')),
            row=1, col=1
        )
    
    # Log buckets
    if len(df_fh) > 0:
        fh_by_lb = df_fh.groupby('log_buckets')['time_per_email_ms'].mean().reset_index()
        fig_html.add_trace(
            go.Scatter(x=fh_by_lb['log_buckets'], y=fh_by_lb['time_per_email_ms'],
                      mode='lines+markers', name='Feature Hashing', showlegend=False,
                      line=dict(color='steelblue'), marker=dict(size=10)),
            row=1, col=2
        )
    
    if len(df_cm) > 0:
        cm_by_lb = df_cm.groupby('log_buckets')['time_per_email_ms'].mean().reset_index()
        fig_html.add_trace(
            go.Scatter(x=cm_by_lb['log_buckets'], y=cm_by_lb['time_per_email_ms'],
                      mode='lines+markers', name='Count-Min', showlegend=False,
                      line=dict(color='forestgreen', dash='dash'), marker=dict(size=10, symbol='square')),
            row=1, col=2
        )
    
    # Num hashes
    if len(df_cm) > 0:
        cm_by_nh = df_cm.groupby('num_hashes')['time_per_email_ms'].mean().reset_index()
        fig_html.add_trace(
            go.Scatter(x=cm_by_nh['num_hashes'], y=cm_by_nh['time_per_email_ms'],
                      mode='lines+markers', name='Count-Min', showlegend=False,
                      line=dict(color='forestgreen'), marker=dict(size=10, symbol='square')),
            row=1, col=3
        )
    
    fig_html.update_xaxes(title_text="N-gram Size", row=1, col=1)
    fig_html.update_xaxes(title_text="Log Num Buckets", row=1, col=2)
    fig_html.update_xaxes(title_text="Num Hashes", row=1, col=3)
    fig_html.update_yaxes(title_text="Avg Time (ms)", row=1, col=1)
    fig_html.update_yaxes(title_text="Avg Time (ms)", row=1, col=2)
    fig_html.update_yaxes(title_text="Avg Time (ms)", row=1, col=3)
    
    fig_html.update_layout(
        title="Timing by Hyperparameter Dimension",
        height=450,
        width=1200
    )
    _save_html(fig_html, "timing_by_dimension.html")
    print("  Saved: reports/timing_by_dimension.html")
    
    # Print summary
    print("\n  === Timing Summary by Dimension ===")
    
    print("\n  By N-gram:")
    for ng in sorted(df['ngram'].unique()):
        fh_time = df_fh[df_fh['ngram'] == ng]['time_per_email_ms'].mean() if len(df_fh) > 0 else float('nan')
        cm_time = df_cm[df_cm['ngram'] == ng]['time_per_email_ms'].mean() if len(df_cm) > 0 else float('nan')
        print(f"    ng={int(ng)}: FH={fh_time:.3f}ms, CM={cm_time:.3f}ms")
    
    print("\n  By Log Buckets:")
    for lb in sorted(df['log_buckets'].unique()):
        fh_time = df_fh[df_fh['log_buckets'] == lb]['time_per_email_ms'].mean() if len(df_fh) > 0 else float('nan')
        cm_time = df_cm[df_cm['log_buckets'] == lb]['time_per_email_ms'].mean() if len(df_cm) > 0 else float('nan')
        print(f"    lb={int(lb)}: FH={fh_time:.3f}ms, CM={cm_time:.3f}ms")
    
    if len(df_cm) > 0:
        print("\n  By Num Hashes (Count-Min only):")
        for nh in sorted(df_cm['num_hashes'].unique()):
            cm_time = df_cm[df_cm['num_hashes'] == nh]['time_per_email_ms'].mean()
            print(f"    nh={int(nh)}: CM={cm_time:.3f}ms")


def plot_space_usage():
    """
    Plot theoretical space usage vs log_buckets.
    - Feature Hashing: 2 arrays (spam/ham) x 2^log_buckets x 8 bytes
    - Count-Min: 2 arrays (spam/ham) x num_hashes x 2^log_buckets x 8 bytes
    """
    print("Plotting space usage...")
    
    # Parameters
    log_buckets_range = np.arange(8, 22)  # 8 to 21
    num_hashes_options = [2, 3, 4, 5]
    bytes_per_counter = 8  # double precision
    
    # Calculate space for Feature Hashing
    # 2 arrays (spam counts, ham counts) x 2^log_buckets counters x 8 bytes
    fh_space = 2 * (2 ** log_buckets_range) * bytes_per_counter
    
    # Calculate space for Count-Min (for each num_hashes)
    # 2 arrays x num_hashes rows x 2^log_buckets counters x 8 bytes
    cm_space = {}
    for nh in num_hashes_options:
        cm_space[nh] = 2 * nh * (2 ** log_buckets_range) * bytes_per_counter
    
    # === Matplotlib: Line plot ===
    fig, ax = plt.subplots(figsize=(12, 7))
    
    # Feature Hashing
    ax.plot(log_buckets_range, fh_space / 1024, 'o-', linewidth=2, markersize=6,
           color='steelblue', label='Feature Hashing')
    
    # Count-Min for different num_hashes
    cm_colors = plt.cm.Greens(np.linspace(0.4, 0.9, len(num_hashes_options)))
    for i, nh in enumerate(num_hashes_options):
        ax.plot(log_buckets_range, cm_space[nh] / 1024, 's--', linewidth=2, markersize=5,
               color=cm_colors[i], label=f'Count-Min (nh={nh})')
    
    ax.set_xlabel('Log Num Buckets')
    ax.set_ylabel('Space Usage (KB)')
    ax.set_title('Memory Usage vs Log Num Buckets')
    ax.legend()
    ax.grid(True, alpha=0.3)
    ax.set_xticks(log_buckets_range)
    
    # Add secondary y-axis in MB for large values
    ax2 = ax.secondary_yaxis('right', functions=(lambda x: x/1024, lambda x: x*1024))
    ax2.set_ylabel('Space Usage (MB)')
    
    plt.tight_layout()
    _save_png(fig, 'space_usage.png')
    print("  Saved: reports/space_usage.png")
    
    # === Log scale version ===
    fig, ax = plt.subplots(figsize=(12, 7))
    
    ax.semilogy(log_buckets_range, fh_space, 'o-', linewidth=2, markersize=6,
               color='steelblue', label='Feature Hashing')
    
    for i, nh in enumerate(num_hashes_options):
        ax.semilogy(log_buckets_range, cm_space[nh], 's--', linewidth=2, markersize=5,
                   color=cm_colors[i], label=f'Count-Min (nh={nh})')
    
    ax.set_xlabel('Log Num Buckets')
    ax.set_ylabel('Space Usage (bytes, log scale)')
    ax.set_title('Memory Usage vs Log Num Buckets (Log Scale)')
    ax.legend()
    ax.grid(True, alpha=0.3, which='both')
    ax.set_xticks(log_buckets_range)
    
    # Add reference lines for common memory sizes
    memory_refs = [(1024, '1 KB'), (1024**2, '1 MB'), (1024**3, '1 GB')]
    for size, label in memory_refs:
        if fh_space.min() < size < cm_space[max(num_hashes_options)].max():
            ax.axhline(y=size, color='red', linestyle=':', alpha=0.5, linewidth=1)
            ax.text(log_buckets_range[-1] + 0.3, size, label, va='center', fontsize=9, color='red')
    
    plt.tight_layout()
    _save_png(fig, 'space_usage_log.png')
    print("  Saved: reports/space_usage_log.png")
    
    # === Plotly interactive ===
    fig_html = go.Figure()
    
    fig_html.add_trace(
        go.Scatter(x=log_buckets_range, y=fh_space,
                  mode='lines+markers', name='Feature Hashing',
                  line=dict(color='steelblue'),
                  hovertemplate='log_buckets=%{x}<br>Space: %{y:,.0f} bytes<br>(%{customdata:.2f} KB)',
                  customdata=fh_space/1024)
    )
    
    for i, nh in enumerate(num_hashes_options):
        fig_html.add_trace(
            go.Scatter(x=log_buckets_range, y=cm_space[nh],
                      mode='lines+markers', name=f'Count-Min (nh={nh})',
                      line=dict(dash='dash'),
                      hovertemplate=f'log_buckets=%{{x}}<br>nh={nh}<br>Space: %{{y:,.0f}} bytes<br>(%{{customdata:.2f}} KB)',
                      customdata=cm_space[nh]/1024)
        )
    
    fig_html.update_layout(
        title="Memory Usage vs Log Num Buckets",
        xaxis_title="Log Num Buckets",
        yaxis_title="Space Usage (bytes)",
        yaxis_type="log",
        height=600,
        width=900
    )
    
    # Add reference lines
    for size, label in memory_refs:
        fig_html.add_hline(y=size, line_dash="dot", line_color="red", opacity=0.5,
                          annotation_text=label, annotation_position="right")
    
    _save_html(fig_html, "space_usage.html")
    print("  Saved: reports/space_usage.html")
    
    # Print summary table
    print("\n  === Space Usage Summary ===")
    print(f"  {'log_buckets':<12} {'FH (KB)':<12} " + " ".join([f'CM nh={nh} (KB)' for nh in num_hashes_options]))
    print("  " + "-" * 70)
    for i, lb in enumerate(log_buckets_range):
        fh_kb = fh_space[i] / 1024
        cm_kbs = [cm_space[nh][i] / 1024 for nh in num_hashes_options]
        print(f"  {lb:<12} {fh_kb:<12.1f} " + " ".join([f'{kb:<12.1f}' for kb in cm_kbs]))


def plot_parameter_sweep():
    """
    Plot metrics (Precision, Recall, F1, F0.5, Accuracy) vs each parameter.
    Shows average metric for each parameter value across all other parameters.
    """
    print("Plotting parameter sweep results...")
    
    try:
        df = pd.read_csv('parameter_sweep.csv')
    except FileNotFoundError:
        print("  Warning: parameter_sweep.csv not found")
        return
    
    print(f"  Loaded {len(df)} parameter sweep data points")
    
    metrics = ['precision', 'recall', 'f1', 'f05', 'accuracy']
    metric_labels = ['Precision', 'Recall', 'F1', 'F0.5', 'Accuracy']
    
    # Separate classifiers
    df_fh = df[df['classifier'] == 'FeatureHashing']
    df_cm = df[df['classifier'] == 'CountMin']
    
    # Parameters to sweep
    params = [
        ('ngram', 'N-gram Size', sorted(df['ngram'].unique())),
        ('log_buckets', 'Log(Num Buckets)', sorted(df['log_buckets'].unique())),
    ]
    
    # Add num_hashes for Count-Min only
    if len(df_cm) > 0:
        params.append(('num_hashes', 'Num Hashes', sorted(df_cm['num_hashes'].unique())))
    
    # === Matplotlib: Grid of metrics vs each parameter ===
    for param_name, param_label, param_values in params:
        fig, axes = plt.subplots(2, 3, figsize=(16, 10))
        axes = axes.flatten()
        
        for idx, (metric, m_label) in enumerate(zip(metrics, metric_labels)):
            ax = axes[idx]
            
            # Feature Hashing (skip num_hashes since FH doesn't have it)
            if param_name != 'num_hashes' and len(df_fh) > 0:
                fh_by_param = df_fh.groupby(param_name)[metric].agg(['mean', 'std']).reset_index()
                ax.errorbar(fh_by_param[param_name], fh_by_param['mean'], 
                           yerr=fh_by_param['std'], fmt='o-', capsize=4,
                           linewidth=2, markersize=8, color='steelblue', 
                           label='Feature Hashing')
            
            # Count-Min
            if len(df_cm) > 0:
                cm_by_param = df_cm.groupby(param_name)[metric].agg(['mean', 'std']).reset_index()
                ax.errorbar(cm_by_param[param_name], cm_by_param['mean'],
                           yerr=cm_by_param['std'], fmt='s--', capsize=4,
                           linewidth=2, markersize=8, color='forestgreen',
                           label='Count-Min')
            
            ax.set_xlabel(param_label)
            ax.set_ylabel(m_label)
            ax.set_title(f'{m_label} vs {param_label}')
            ax.legend(loc='best')
            ax.grid(True, alpha=0.3)
            ax.set_xticks(param_values)
        
        # Hide unused subplot
        axes[-1].set_visible(False)
        
        plt.tight_layout()
        _save_png(fig, f'param_sweep_{param_name}.png')
        print(f"  Saved: reports/param_sweep_{param_name}.png")
    
    # === Matplotlib: Combined plot - all metrics vs all params ===
    fig, axes = plt.subplots(1, len(params), figsize=(6*len(params), 6))
    if len(params) == 1:
        axes = [axes]
    
    colors = plt.cm.Set1(np.linspace(0, 1, len(metrics)))
    markers = ['o', 's', '^', 'D', 'v']
    
    for ax_idx, (param_name, param_label, param_values) in enumerate(params):
        ax = axes[ax_idx]
        
        # Plot each metric for Feature Hashing (solid lines)
        if param_name != 'num_hashes' and len(df_fh) > 0:
            for m_idx, (metric, m_label) in enumerate(zip(metrics, metric_labels)):
                fh_by_param = df_fh.groupby(param_name)[metric].mean()
                ax.plot(fh_by_param.index, fh_by_param.values,
                       marker=markers[m_idx], linestyle='-', linewidth=2, markersize=6,
                       color=colors[m_idx], label=f'FH: {m_label}', alpha=0.8)
        
        # Plot each metric for Count-Min (dashed lines)
        if len(df_cm) > 0:
            for m_idx, (metric, m_label) in enumerate(zip(metrics, metric_labels)):
                cm_by_param = df_cm.groupby(param_name)[metric].mean()
                ax.plot(cm_by_param.index, cm_by_param.values,
                       marker=markers[m_idx], linestyle='--', linewidth=2, markersize=6,
                       color=colors[m_idx], label=f'CM: {m_label}', alpha=0.8)
        
        ax.set_xlabel(param_label)
        ax.set_ylabel('Score')
        ax.set_title(f'Metrics vs {param_label}')
        ax.legend(loc='best', fontsize=7, ncol=2)
        ax.grid(True, alpha=0.3)
        ax.set_xticks(param_values)
        ax.set_ylim([0.5, 1.0])
    
    plt.tight_layout()
    _save_png(fig, 'param_sweep_combined.png')
    print("  Saved: reports/param_sweep_combined.png")
    
    # === Plotly: Interactive subplots for each parameter ===
    for param_name, param_label, param_values in params:
        fig_html = make_subplots(
            rows=2, cols=3,
            subplot_titles=metric_labels + [''],
            horizontal_spacing=0.08,
            vertical_spacing=0.12
        )
        
        for idx, (metric, m_label) in enumerate(zip(metrics, metric_labels)):
            row = idx // 3 + 1
            col = idx % 3 + 1
            
            # Feature Hashing
            if param_name != 'num_hashes' and len(df_fh) > 0:
                fh_agg = df_fh.groupby(param_name)[metric].agg(['mean', 'std']).reset_index()
                fig_html.add_trace(
                    go.Scatter(
                        x=fh_agg[param_name],
                        y=fh_agg['mean'],
                        error_y=dict(type='data', array=fh_agg['std'], visible=True),
                        mode='lines+markers',
                        name='Feature Hashing',
                        legendgroup='FH',
                        showlegend=(idx == 0),
                        line=dict(color='steelblue'),
                        marker=dict(size=10)
                    ),
                    row=row, col=col
                )
            
            # Count-Min
            if len(df_cm) > 0:
                cm_agg = df_cm.groupby(param_name)[metric].agg(['mean', 'std']).reset_index()
                fig_html.add_trace(
                    go.Scatter(
                        x=cm_agg[param_name],
                        y=cm_agg['mean'],
                        error_y=dict(type='data', array=cm_agg['std'], visible=True),
                        mode='lines+markers',
                        name='Count-Min',
                        legendgroup='CM',
                        showlegend=(idx == 0),
                        line=dict(color='forestgreen', dash='dash'),
                        marker=dict(size=10, symbol='square')
                    ),
                    row=row, col=col
                )
            
            fig_html.update_xaxes(title_text=param_label, row=row, col=col)
            fig_html.update_yaxes(title_text=m_label, row=row, col=col)
        
        fig_html.update_layout(
            title=f"Performance Metrics vs {param_label}"
        )
        _save_html(fig_html, f"param_sweep_{param_name}.html")
        print(f"  Saved: reports/param_sweep_{param_name}.html")
    
    # === Plotly: Single interactive with dropdown for parameter ===
    fig_combined = go.Figure()
    
    # Build traces for each parameter
    all_traces = []
    buttons = []
    
    for p_idx, (param_name, param_label, param_values) in enumerate(params):
        param_traces = []
        
        for m_idx, (metric, m_label) in enumerate(zip(metrics, metric_labels)):
            # Feature Hashing
            if param_name != 'num_hashes' and len(df_fh) > 0:
                fh_agg = df_fh.groupby(param_name)[metric].mean().reset_index()
                param_traces.append(go.Scatter(
                    x=fh_agg[param_name],
                    y=fh_agg[metric],
                    mode='lines+markers',
                    name=f'FH: {m_label}',
                    visible=(p_idx == 0),
                    line=dict(width=2),
                    marker=dict(size=8)
                ))
            
            # Count-Min
            if len(df_cm) > 0:
                cm_agg = df_cm.groupby(param_name)[metric].mean().reset_index()
                param_traces.append(go.Scatter(
                    x=cm_agg[param_name],
                    y=cm_agg[metric],
                    mode='lines+markers',
                    name=f'CM: {m_label}',
                    visible=(p_idx == 0),
                    line=dict(width=2, dash='dash'),
                    marker=dict(size=8, symbol='square')
                ))
        
        all_traces.extend(param_traces)
        
        # Calculate visibility for this parameter
        traces_per_param = len(metrics) * (1 if param_name == 'num_hashes' else 2)
        if param_name != 'num_hashes':
            traces_per_param = len(metrics) * 2  # FH + CM
        else:
            traces_per_param = len(metrics)  # CM only
    
    for trace in all_traces:
        fig_combined.add_trace(trace)
    
    # Calculate traces per parameter for visibility toggling
    traces_before = 0
    for p_idx, (param_name, param_label, param_values) in enumerate(params):
        if param_name != 'num_hashes':
            n_traces = len(metrics) * 2  # FH + CM for each metric
        else:
            n_traces = len(metrics)  # CM only
        
        visibility = [False] * len(all_traces)
        for i in range(n_traces):
            visibility[traces_before + i] = True
        
        buttons.append(dict(
            label=param_label,
            method='update',
            args=[{'visible': visibility},
                  {'title': f'Metrics vs {param_label}',
                   'xaxis': {'title': param_label}}]
        ))
        
        traces_before += n_traces
    
    fig_combined.update_layout(
        updatemenus=[dict(
            active=0,
            buttons=buttons,
            direction="down",
            x=0.1,
            y=1.15,
            showactive=True,
        )],
        title="Metrics vs N-gram Size",
        xaxis_title="N-gram Size",
        yaxis_title="Score",
        yaxis=dict(range=[0.5, 1.0])
    )
    _save_html(fig_combined, "param_sweep_interactive.html")
    print("  Saved: reports/param_sweep_interactive.html")
    
    # Print summary
    print("\n  === Parameter Sweep Summary ===")
    for param_name, param_label, param_values in params:
        print(f"\n  By {param_label}:")
        for val in param_values:
            fh_subset = df_fh[df_fh[param_name] == val] if param_name != 'num_hashes' else pd.DataFrame()
            cm_subset = df_cm[df_cm[param_name] == val]
            
            if len(fh_subset) > 0:
                print(f"    {param_name}={val}: FH F0.5={fh_subset['f05'].mean():.4f} "
                      f"P={fh_subset['precision'].mean():.4f} R={fh_subset['recall'].mean():.4f}")
            if len(cm_subset) > 0:
                print(f"    {param_name}={val}: CM F0.5={cm_subset['f05'].mean():.4f} "
                      f"P={cm_subset['precision'].mean():.4f} R={cm_subset['recall'].mean():.4f}")


def plot_brier_scores():
    """
    Plot Brier scores and Log Loss for final configurations.
    Lower Brier score = better calibrated probabilities.
    """
    print("Plotting Brier scores...")
    
    try:
        df = pd.read_csv('brier_scores.csv')
    except FileNotFoundError:
        print("  Warning: brier_scores.csv not found")
        return
    
    print(f"  Loaded {len(df)} configurations")
    
    # Create configuration labels
    labels = [_config_label(row) for _, row in df.iterrows()]
    
    # === Matplotlib: Bar chart of Brier scores ===
    fig, axes = plt.subplots(1, 3, figsize=(18, 6))
    
    colors = ['steelblue' if row['classifier'] == 'FeatureHashing' else 'forestgreen' 
              for _, row in df.iterrows()]
    
    # Brier Score (lower is better)
    ax = axes[0]
    x = np.arange(len(labels))
    bars = ax.bar(x, df['brier_score'], color=colors, alpha=0.8)
    ax.set_xticks(x)
    ax.set_xticklabels(labels, rotation=45, ha='right', fontsize=8)
    ax.set_ylabel('Brier Score')
    ax.set_title('Brier Score (lower = better calibration)')
    ax.grid(True, alpha=0.3, axis='y')
    
    # Add value labels
    for bar, val in zip(bars, df['brier_score']):
        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.002,
               f'{val:.4f}', ha='center', va='bottom', fontsize=7, rotation=90)
    
    # Brier Skill Score (higher is better, relative to climatology)
    ax = axes[1]
    bars = ax.bar(x, df['brier_score_scaled'], color=colors, alpha=0.8)
    ax.set_xticks(x)
    ax.set_xticklabels(labels, rotation=45, ha='right', fontsize=8)
    ax.set_ylabel('Brier Skill Score')
    ax.set_title('Brier Skill Score (higher = better vs baseline)')
    ax.grid(True, alpha=0.3, axis='y')
    ax.axhline(y=0, color='red', linestyle='--', alpha=0.5)
    
    for bar, val in zip(bars, df['brier_score_scaled']):
        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
               f'{val:.3f}', ha='center', va='bottom', fontsize=7, rotation=90)
    
    # Log Loss (lower is better)
    ax = axes[2]
    bars = ax.bar(x, df['log_loss'], color=colors, alpha=0.8)
    ax.set_xticks(x)
    ax.set_xticklabels(labels, rotation=45, ha='right', fontsize=8)
    ax.set_ylabel('Log Loss')
    ax.set_title('Log Loss (lower = better)')
    ax.grid(True, alpha=0.3, axis='y')
    
    for bar, val in zip(bars, df['log_loss']):
        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
               f'{val:.3f}', ha='center', va='bottom', fontsize=7, rotation=90)
    
    # Add legend
    from matplotlib.patches import Patch
    legend_elements = [Patch(facecolor='steelblue', alpha=0.8, label='Feature Hashing'),
                      Patch(facecolor='forestgreen', alpha=0.8, label='Count-Min')]
    axes[0].legend(handles=legend_elements, loc='upper right')
    
    plt.tight_layout()
    _save_png(fig, 'brier_scores.png')
    print("  Saved: reports/brier_scores.png")
    
    # === Plotly: Interactive bar chart ===
    fig_html = make_subplots(
        rows=1, cols=3,
        subplot_titles=['Brier Score (lower=better)', 
                       'Brier Skill Score (higher=better)',
                       'Log Loss (lower=better)'],
        horizontal_spacing=0.08
    )
    
    # Brier Score
    fig_html.add_trace(
        go.Bar(
            x=labels,
            y=df['brier_score'],
            marker_color=['steelblue' if c == 'FeatureHashing' else 'forestgreen' 
                         for c in df['classifier']],
            name='Brier Score',
            showlegend=False,
            hovertemplate='%{x}<br>Brier Score: %{y:.4f}<extra></extra>'
        ),
        row=1, col=1
    )
    
    # Brier Skill Score
    fig_html.add_trace(
        go.Bar(
            x=labels,
            y=df['brier_score_scaled'],
            marker_color=['steelblue' if c == 'FeatureHashing' else 'forestgreen' 
                         for c in df['classifier']],
            name='Brier Skill',
            showlegend=False,
            hovertemplate='%{x}<br>Brier Skill: %{y:.4f}<extra></extra>'
        ),
        row=1, col=2
    )
    
    # Log Loss
    fig_html.add_trace(
        go.Bar(
            x=labels,
            y=df['log_loss'],
            marker_color=['steelblue' if c == 'FeatureHashing' else 'forestgreen' 
                         for c in df['classifier']],
            name='Log Loss',
            showlegend=False,
            hovertemplate='%{x}<br>Log Loss: %{y:.4f}<extra></extra>'
        ),
        row=1, col=3
    )
    
    fig_html.update_layout(
        title="Probability Calibration Metrics"
    )
    _save_html(fig_html, "brier_scores.html")
    print("  Saved: reports/brier_scores.html")
    
    # Print summary
    print("\n  === Brier Score Summary ===")
    print(f"  {'Configuration':<45} {'Brier':>8} {'Skill':>8} {'LogLoss':>8}")
    print("  " + "-" * 75)
    
    for _, row in df.iterrows():
        label = _config_label(row)
        print(f"  {label:<45} {row['brier_score']:>8.4f} {row['brier_score_scaled']:>8.4f} {row['log_loss']:>8.4f}")
    
    # Best configurations
    best_brier = df.loc[df['brier_score'].idxmin()]
    best_skill = df.loc[df['brier_score_scaled'].idxmax()]
    best_logloss = df.loc[df['log_loss'].idxmin()]
    
    print(f"\n  Best Brier Score: {_config_label(best_brier)} = {best_brier['brier_score']:.4f}")
    print(f"  Best Brier Skill: {_config_label(best_skill)} = {best_skill['brier_score_scaled']:.4f}")
    print(f"  Best Log Loss: {_config_label(best_logloss)} = {best_logloss['log_loss']:.4f}")


def plot_metrics_vs_dimension():
    """
    Plot metrics (Recall@Prec95, Recall@Prec99, AUC) vs dimension (2^log_buckets).
    Shows how performance scales with model size.
    """
    print("Plotting metrics vs dimension...")
    
    try:
        df = pd.read_csv('cv_results.csv')
    except FileNotFoundError:
        print("  Warning: cv_results.csv not found")
        return
    
    print(f"  Loaded {len(df)} CV data points")
    
    # Average across folds for each configuration
    agg_cols = {
        'recall_at_prec_95': 'mean',
        'recall_at_prec_99': 'mean',
        'auc': 'mean',
        'recall_at_fpr_001': 'mean',
        'recall_at_fpr_0001': 'mean'
    }
    if 'brier_score' in df.columns:
        agg_cols['brier_score'] = 'mean'
    agg_df = df.groupby(['classifier', 'ngram', 'log_buckets', 'num_hashes']).agg(agg_cols).reset_index()
    
    # Calculate effective dimension (number of counters)
    # FH: 2^log_buckets, CM: num_hashes * 2^log_buckets
    agg_df['dimension'] = np.where(
        agg_df['classifier'] == 'FeatureHashing',
        2 ** agg_df['log_buckets'],
        agg_df['num_hashes'] * (2 ** agg_df['log_buckets'])
    )
    
    # Also keep raw log_buckets for x-axis
    agg_df['buckets'] = 2 ** agg_df['log_buckets']
    
    # Build metrics list - include Brier if available (inverted so higher is better)
    metrics = ['recall_at_prec_95', 'recall_at_prec_99', 'auc', 'recall_at_fpr_001']
    metric_labels = ['Recall @ Precision=0.95', 'Recall @ Precision=0.99', 'AUC', 'Recall @ FPR=0.001']
    
    if 'brier_score' in agg_df.columns:
        # Create inverted brier (1 - brier) so higher is better
        agg_df['brier_skill'] = 1 - agg_df['brier_score']
        metrics.append('brier_skill')
        metric_labels.append('1 - Brier Score')
    
    # Separate classifiers
    df_fh = agg_df[agg_df['classifier'] == 'FeatureHashing']
    df_cm = agg_df[agg_df['classifier'] == 'CountMin']
    
    # === Matplotlib: Grid of metrics vs log_buckets ===
    n_metrics = len(metrics)
    n_cols = 3
    n_rows = (n_metrics + n_cols - 1) // n_cols
    fig, axes = plt.subplots(n_rows, n_cols, figsize=(16, 5*n_rows))
    axes = axes.flatten()
    
    for idx, (metric, label) in enumerate(zip(metrics, metric_labels)):
        ax = axes[idx]
        
        # Feature Hashing: group by log_buckets, average across ngrams
        if len(df_fh) > 0:
            fh_by_lb = df_fh.groupby('log_buckets')[metric].mean()
            ax.plot(fh_by_lb.index, fh_by_lb.values, 'o-', 
                   linewidth=2, markersize=8, color='steelblue', label='Feature Hashing')
            
            # Also show individual ngram lines with transparency
            for ng in sorted(df_fh['ngram'].unique()):
                subset = df_fh[df_fh['ngram'] == ng].sort_values('log_buckets')
                ax.plot(subset['log_buckets'], subset[metric], 
                       alpha=0.3, linewidth=1, color='steelblue')
        
        # Count-Min: group by log_buckets, average across ngrams and num_hashes
        if len(df_cm) > 0:
            cm_by_lb = df_cm.groupby('log_buckets')[metric].mean()
            ax.plot(cm_by_lb.index, cm_by_lb.values, 's--', 
                   linewidth=2, markersize=8, color='forestgreen', label='Count-Min')
            
            # Individual lines
            for ng in sorted(df_cm['ngram'].unique()):
                for nh in sorted(df_cm['num_hashes'].unique()):
                    subset = df_cm[(df_cm['ngram'] == ng) & (df_cm['num_hashes'] == nh)]
                    subset = subset.sort_values('log_buckets')
                    if len(subset) > 1:
                        ax.plot(subset['log_buckets'], subset[metric], 
                               alpha=0.2, linewidth=1, color='forestgreen')
        
        ax.set_xlabel('Log(Num Buckets)')
        ax.set_ylabel(label)
        ax.set_title(f'{label} vs Dimension')
        ax.legend(loc='lower right')
        ax.grid(True, alpha=0.3)
        ax.set_ylim([0, 1])
    
    # Hide unused subplots
    for idx in range(len(metrics), len(axes)):
        axes[idx].set_visible(False)
    
    plt.tight_layout()
    _save_png(fig, 'metrics_vs_dimension.png')
    print("  Saved: reports/metrics_vs_dimension.png")
    
    # === Matplotlib: Single plot with all metrics ===
    fig, ax = plt.subplots(figsize=(12, 7))
    
    colors = plt.cm.Set1(np.linspace(0, 1, len(metrics)))
    markers = ['o', 's', '^', 'D']
    
    for i, (metric, label) in enumerate(zip(metrics, metric_labels)):
        # Feature Hashing average
        if len(df_fh) > 0:
            fh_by_lb = df_fh.groupby('log_buckets')[metric].mean()
            ax.plot(fh_by_lb.index, fh_by_lb.values, 
                   marker=markers[i], linestyle='-', linewidth=2, markersize=8,
                   color=colors[i], label=f'FH: {label}')
    
    ax.set_xlabel('Log(Num Buckets)')
    ax.set_ylabel('Score')
    ax.set_title('All Metrics vs Dimension (Feature Hashing)')
    ax.legend(loc='lower right', fontsize=9)
    ax.grid(True, alpha=0.3)
    ax.set_ylim([0, 1])
    
    plt.tight_layout()
    _save_png(fig, 'metrics_vs_dimension_combined.png')
    print("  Saved: reports/metrics_vs_dimension_combined.png")
    
    # === Plotly: Interactive with dimension on x-axis ===
    fig_html = go.Figure()
    
    # Feature Hashing traces
    if len(df_fh) > 0:
        for metric, label in zip(metrics, metric_labels):
            fh_by_lb = df_fh.groupby('log_buckets')[metric].mean().reset_index()
            fh_by_lb['buckets'] = 2 ** fh_by_lb['log_buckets']
            
            fig_html.add_trace(
                go.Scatter(
                    x=fh_by_lb['log_buckets'],
                    y=fh_by_lb[metric],
                    mode='lines+markers',
                    name=f'FH: {label}',
                    hovertemplate=f'log_buckets=%{{x}}<br>buckets=%{{customdata:,}}<br>{label}=%{{y:.4f}}',
                    customdata=fh_by_lb['buckets']
                )
            )
    
    # Count-Min traces
    if len(df_cm) > 0:
        for metric, label in zip(metrics, metric_labels):
            cm_by_lb = df_cm.groupby('log_buckets')[metric].mean().reset_index()
            cm_by_lb['buckets'] = 2 ** cm_by_lb['log_buckets']
            
            fig_html.add_trace(
                go.Scatter(
                    x=cm_by_lb['log_buckets'],
                    y=cm_by_lb[metric],
                    mode='lines+markers',
                    name=f'CM: {label}',
                    line=dict(dash='dash'),
                    hovertemplate=f'log_buckets=%{{x}}<br>buckets=%{{customdata:,}}<br>{label}=%{{y:.4f}}',
                    customdata=cm_by_lb['buckets']
                )
            )
    
    fig_html.update_layout(
        title="Metrics vs Dimension (Log Num Buckets)",
        xaxis_title="Log(Num Buckets)",
        yaxis_title="Score",
        yaxis=dict(range=[0, 1]),
        legend=dict(font=dict(size=10))
    )
    _save_html(fig_html, "metrics_vs_dimension.html")
    print("  Saved: reports/metrics_vs_dimension.html")
    
    # === Plotly: Subplots for each metric ===
    fig_sub = make_subplots(
        rows=2, cols=2,
        subplot_titles=metric_labels,
        horizontal_spacing=0.1,
        vertical_spacing=0.12
    )
    
    for idx, (metric, label) in enumerate(zip(metrics, metric_labels)):
        row = idx // 2 + 1
        col = idx % 2 + 1
        
        if len(df_fh) > 0:
            fh_by_lb = df_fh.groupby('log_buckets')[metric].mean().reset_index()
            fig_sub.add_trace(
                go.Scatter(
                    x=fh_by_lb['log_buckets'],
                    y=fh_by_lb[metric],
                    mode='lines+markers',
                    name='Feature Hashing',
                    legendgroup='FH',
                    showlegend=(idx == 0),
                    line=dict(color='steelblue'),
                    marker=dict(size=8)
                ),
                row=row, col=col
            )
        
        if len(df_cm) > 0:
            cm_by_lb = df_cm.groupby('log_buckets')[metric].mean().reset_index()
            fig_sub.add_trace(
                go.Scatter(
                    x=cm_by_lb['log_buckets'],
                    y=cm_by_lb[metric],
                    mode='lines+markers',
                    name='Count-Min',
                    legendgroup='CM',
                    showlegend=(idx == 0),
                    line=dict(color='forestgreen', dash='dash'),
                    marker=dict(size=8, symbol='square')
                ),
                row=row, col=col
            )
        
        fig_sub.update_xaxes(title_text="Log(Buckets)", row=row, col=col)
        fig_sub.update_yaxes(title_text="Score", range=[0, 1], row=row, col=col)
    
    fig_sub.update_layout(
        title="Performance Metrics vs Model Dimension"
    )
    _save_html(fig_sub, "metrics_vs_dimension_grid.html")
    print("  Saved: reports/metrics_vs_dimension_grid.html")
    
    # Print summary
    print("\n  === Metrics vs Dimension Summary ===")
    for lb in sorted(agg_df['log_buckets'].unique()):
        print(f"\n  log_buckets={int(lb)} (buckets={2**int(lb):,}):")
        
        fh_subset = df_fh[df_fh['log_buckets'] == lb]
        if len(fh_subset) > 0:
            print(f"    FH: Rec@P95={fh_subset['recall_at_prec_95'].mean():.4f}, "
                  f"AUC={fh_subset['auc'].mean():.4f}")
        
        cm_subset = df_cm[df_cm['log_buckets'] == lb]
        if len(cm_subset) > 0:
            print(f"    CM: Rec@P95={cm_subset['recall_at_prec_95'].mean():.4f}, "
                  f"AUC={cm_subset['auc'].mean():.4f}")


def main():
    """
    Run all visualization functions for selected configurations.
    """
    print("=" * 60)
    print("Spam Filter Visualization")
    print("=" * 60)
    
    # Create output directory
    OUTPUT_DIR.mkdir(exist_ok=True)
    
    # Cross-validation plots
    plot_cv_recall_at_precision()
    print()
    
    plot_metrics_vs_dimension()
    print()
    
    plot_parameter_sweep()
    print()

    # Space and timing analysis
    plot_space_usage()
    print()
    
    plot_timing_4d()
    print()
    
    plot_timing_by_dimension()
    print()
    
    # Selected configuration plots
    plot_pr_curves_selected()
    print()
    
    plot_roc_curves_selected()
    print()
    
    plot_learning_curves_selected()
    print()
    
    plot_test_vs_dev()
    print()
    
    plot_config_summary()
    print()
    
    plot_brier_scores()
    
    print("\n" + "=" * 60)
    print("All visualizations complete!")
    print(f"Output saved to: {OUTPUT_DIR.absolute()}")
    print("=" * 60)


if __name__ == '__main__':
    main()
