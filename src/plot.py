#!/usr/bin/env python3
"""
Visualization script for spam filter experiments.
Reads CSV files generated by the C++ experiments and creates plots
for the SELECTED configurations only (not cross-validation data).
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
from pathlib import Path
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots

# Set style for better-looking plots
sns.set_style("whitegrid")
sns.set_palette("husl")
plt.rcParams['figure.figsize'] = (12, 8)
plt.rcParams['font.size'] = 10

OUTPUT_DIR = Path("reports")
OUTPUT_DIR.mkdir(exist_ok=True)

# Baseline precision for spam filtering (proportion of spam in dataset)
BASELINE_PRECISION = 0.609205


def _save_png(fig, name: str):
    """Save matplotlib figure to reports/."""
    fig.savefig(OUTPUT_DIR / name, dpi=300, bbox_inches='tight')
    plt.close(fig)


def _save_html(fig, name: str):
    """Save plotly figure to reports/."""
    fig.write_html(OUTPUT_DIR / name, include_plotlyjs="cdn")


def _config_label(row, include_opt=True):
    """Create a descriptive label for a configuration."""
    clf = row['classifier']
    ng = int(row['ngram'])
    lb = int(row['log_buckets'])
    nh = int(row['num_hashes']) if 'num_hashes' in row else 0
    
    if clf == 'FeatureHashing':
        label = f"FH: ng={ng}, lb={lb}"
    else:
        label = f"CM: ng={ng}, lb={lb}, nh={nh}"
    
    if include_opt and 'optimized_for' in row:
        label += f" ({row['optimized_for']})"
    
    return label


def plot_pr_curves_selected():
    """
    Plot PR curves from Phase 3 threshold tuning for selected configurations.
    Creates combined PR curve and F0.5 vs threshold plot.
    """
    print("Plotting PR curves for selected configurations...")
    
    try:
        df = pd.read_csv('pr_curve.csv')
    except FileNotFoundError:
        print("  Warning: pr_curve_phase3.csv not found")
        return
    
    # Get unique configurations
    config_cols = ['classifier', 'ngram', 'log_buckets', 'num_hashes', 'optimized_for']
    configs = df.groupby(config_cols).size().reset_index()[config_cols]
    
    print(f"  Found {len(configs)} configurations")
    
    # Color palettes for classifiers
    fh_configs = configs[configs['classifier'] == 'FeatureHashing']
    cm_configs = configs[configs['classifier'] == 'CountMin']
    fh_colors = plt.cm.Blues(np.linspace(0.4, 0.9, max(len(fh_configs), 1)))
    cm_colors = plt.cm.Greens(np.linspace(0.4, 0.9, max(len(cm_configs), 1)))
    
    # === Matplotlib: Combined PR and F0.5 plots ===
    fig, axes = plt.subplots(1, 2, figsize=(16, 7))
    
    fh_idx, cm_idx = 0, 0
    
    for _, config in configs.iterrows():
        mask = True
        for col in config_cols:
            mask = mask & (df[col] == config[col])
        df_config = df[mask].copy()
        
        if len(df_config) == 0:
            continue
        
        label = _config_label(config)
        
        if config['classifier'] == 'FeatureHashing':
            color = fh_colors[fh_idx % len(fh_colors)]
            fh_idx += 1
        else:
            color = cm_colors[cm_idx % len(cm_colors)]
            cm_idx += 1
        
        # PR curve
        axes[0].plot(df_config['recall'], df_config['precision'],
                    linewidth=1.5, label=label, alpha=0.8, color=color)
        
        # F0.5 vs threshold
        axes[1].plot(df_config['threshold'], df_config['f05'],
                    linewidth=1.5, label=label, alpha=0.8, color=color)
    
    # Add baseline to PR curve
    axes[0].axhline(y=BASELINE_PRECISION, color='red', linestyle='--', 
                   linewidth=1.5, alpha=0.7, label=f'No-skill (P={BASELINE_PRECISION:.1%})')
    
    # Format PR curve
    axes[0].set_xlabel('Recall (True Positive Rate)')
    axes[0].set_ylabel('Precision')
    axes[0].set_title('PR Curves - Selected Configurations')
    axes[0].legend(loc='lower left', fontsize=7, ncol=2)
    axes[0].grid(True, alpha=0.3)
    axes[0].set_xlim([0, 1])
    axes[0].set_ylim([0, 1])
    
    # Format F0.5 plot
    axes[1].set_xlabel('Threshold')
    axes[1].set_ylabel('F0.5 Score')
    axes[1].set_title('F0.5 vs Threshold - Selected Configurations')
    axes[1].legend(loc='lower right', fontsize=7, ncol=2)
    axes[1].grid(True, alpha=0.3)
    
    plt.tight_layout()
    _save_png(fig, 'pr_curves_selected.png')
    print("  Saved: reports/pr_curves_selected.png")
    
    # === Plotly: Interactive version ===
    fig_html = make_subplots(rows=1, cols=2,
                             subplot_titles=["PR Curves", "F0.5 vs Threshold"])
    
    for _, config in configs.iterrows():
        mask = True
        for col in config_cols:
            mask = mask & (df[col] == config[col])
        df_config = df[mask].copy()
        
        if len(df_config) == 0:
            continue
        
        label = _config_label(config)
        
        # PR curve
        fig_html.add_trace(
            go.Scatter(x=df_config['recall'], y=df_config['precision'],
                      mode='lines', name=label, legendgroup=label,
                      hovertemplate='Recall: %{x:.3f}<br>Precision: %{y:.3f}<br>Threshold: %{customdata:.3f}',
                      customdata=df_config['threshold']),
            row=1, col=1
        )
        
        # F0.5 vs threshold
        fig_html.add_trace(
            go.Scatter(x=df_config['threshold'], y=df_config['f05'],
                      mode='lines', name=label, legendgroup=label, showlegend=False,
                      hovertemplate='Threshold: %{x:.3f}<br>F0.5: %{y:.3f}'),
            row=1, col=2
        )
    
    # Add baseline
    fig_html.add_trace(
        go.Scatter(x=[0, 1], y=[BASELINE_PRECISION, BASELINE_PRECISION],
                  mode='lines', line=dict(dash='dash', color='red', width=2),
                  name=f'No-skill (P={BASELINE_PRECISION:.1%})'),
        row=1, col=1
    )
    
    fig_html.update_xaxes(title_text="Recall", range=[0, 1], row=1, col=1)
    fig_html.update_yaxes(title_text="Precision", range=[0, 1], row=1, col=1)
    fig_html.update_xaxes(title_text="Threshold", row=1, col=2)
    fig_html.update_yaxes(title_text="F0.5 Score", row=1, col=2)
    fig_html.update_layout(
        title="PR Curves - Selected Configurations",
        height=550, width=1300,
        legend=dict(font=dict(size=9))
    )
    _save_html(fig_html, "pr_curves_selected.html")
    print("  Saved: reports/pr_curves_selected.html")


def plot_roc_curves_selected():
    """
    Plot ROC curves from Phase 3 threshold tuning for selected configurations.
    Computes AUC using trapezoidal rule and displays in legend.
    """
    print("Plotting ROC curves for selected configurations...")
    
    try:
        df = pd.read_csv('roc_curve.csv')
    except FileNotFoundError:
        print("  Warning: roc_curve.csv not found")
        return
    
    # Get unique configurations
    config_cols = ['classifier', 'ngram', 'log_buckets', 'num_hashes', 'optimized_for']
    configs = df.groupby(config_cols).size().reset_index()[config_cols]
    
    print(f"  Found {len(configs)} configurations")
    
    # Color palettes
    fh_configs = configs[configs['classifier'] == 'FeatureHashing']
    cm_configs = configs[configs['classifier'] == 'CountMin']
    fh_colors = plt.cm.Blues(np.linspace(0.4, 0.9, max(len(fh_configs), 1)))
    cm_colors = plt.cm.Greens(np.linspace(0.4, 0.9, max(len(cm_configs), 1)))
    
    # === Matplotlib: ROC curve ===
    fig, ax = plt.subplots(figsize=(10, 9))
    
    fh_idx, cm_idx = 0, 0
    auc_data = []
    
    for _, config in configs.iterrows():
        mask = True
        for col in config_cols:
            mask = mask & (df[col] == config[col])
        df_config = df[mask].copy().sort_values('fpr')
        
        if len(df_config) == 0:
            continue
        
        # Calculate AUC using trapezoidal rule
        auc = np.trapz(df_config['tpr'], df_config['fpr'])
        
        base_label = _config_label(config)
        label = f"{base_label} (AUC={auc:.4f})"
        
        if config['classifier'] == 'FeatureHashing':
            color = fh_colors[fh_idx % len(fh_colors)]
            fh_idx += 1
        else:
            color = cm_colors[cm_idx % len(cm_colors)]
            cm_idx += 1
        
        ax.plot(df_config['fpr'], df_config['tpr'],
               linewidth=1.5, label=label, alpha=0.8, color=color)
        
        auc_data.append({
            'classifier': config['classifier'],
            'ngram': int(config['ngram']),
            'log_buckets': int(config['log_buckets']),
            'num_hashes': int(config['num_hashes']),
            'optimized_for': config['optimized_for'],
            'auc': auc
        })
    
    # Add diagonal (random classifier)
    ax.plot([0, 1], [0, 1], 'k--', linewidth=1, alpha=0.5, label='Random (AUC=0.5)')
    
    ax.set_xlabel('False Positive Rate (FPR)')
    ax.set_ylabel('True Positive Rate (TPR)')
    ax.set_title('ROC Curves - Selected Configurations')
    ax.legend(loc='lower right', fontsize=8)
    ax.grid(True, alpha=0.3)
    ax.set_xlim([0, 1])
    ax.set_ylim([0, 1])
    
    plt.tight_layout()
    _save_png(fig, 'roc_curves_selected.png')
    print("  Saved: reports/roc_curves_selected.png")
    
    # Save AUC data
    auc_df = pd.DataFrame(auc_data).sort_values('auc', ascending=False)
    auc_df.to_csv(OUTPUT_DIR / 'auc_scores_selected.csv', index=False)
    print("  Saved: reports/auc_scores_selected.csv")
    
    # === Plotly: Interactive ROC ===
    fig_html = go.Figure()
    
    for _, config in configs.iterrows():
        mask = True
        for col in config_cols:
            mask = mask & (df[col] == config[col])
        df_config = df[mask].copy().sort_values('fpr')
        
        if len(df_config) == 0:
            continue
        
        auc = np.trapz(df_config['tpr'], df_config['fpr'])
        base_label = _config_label(config)
        label = f"{base_label} (AUC={auc:.4f})"
        
        fig_html.add_trace(
            go.Scatter(x=df_config['fpr'], y=df_config['tpr'],
                      mode='lines', name=label,
                      hovertemplate='FPR: %{x:.3f}<br>TPR: %{y:.3f}<br>Threshold: %{customdata:.3f}',
                      customdata=df_config['threshold'])
        )
    
    # Add diagonal
    fig_html.add_trace(
        go.Scatter(x=[0, 1], y=[0, 1], mode='lines',
                  line=dict(dash='dash', color='black'),
                  name='Random (AUC=0.5)')
    )
    
    fig_html.update_xaxes(title_text="False Positive Rate", range=[0, 1])
    fig_html.update_yaxes(title_text="True Positive Rate", range=[0, 1])
    fig_html.update_layout(
        title="ROC Curves - Selected Configurations",
        height=700, width=900,
        legend=dict(font=dict(size=9))
    )
    _save_html(fig_html, "roc_curves_selected.html")
    print("  Saved: reports/roc_curves_selected.html")


def plot_learning_curves_selected():
    """
    Plot learning curves for selected configurations.
    Shows precision, recall, accuracy, f1, f0.5 over training examples.
    """
    print("Plotting learning curves for selected configurations...")
    
    try:
        df = pd.read_csv('learning_curves.csv')
    except FileNotFoundError:
        print("  Warning: learning_curves.csv not found")
        return
    
    print(f"  Loaded {len(df)} data points")
    
    metrics = ['precision', 'recall', 'accuracy', 'f1', 'f05']
    metric_labels = ['Precision', 'Recall', 'Accuracy', 'F1', 'F0.5']
    
    # Get unique configurations
    config_cols = ['classifier', 'ngram', 'log_buckets', 'num_hashes', 'optimized_for', 'threshold']
    configs = df.groupby(config_cols).size().reset_index()[config_cols]
    
    print(f"  Found {len(configs)} unique configurations")
    
    # Get unique window sizes
    window_sizes = sorted(df['window'].unique())
    
    # === Matplotlib: Grid of metric subplots ===
    fig, axes = plt.subplots(2, 3, figsize=(18, 10))
    axes = axes.flatten()
    
    # Create color map for configs
    colors = plt.cm.tab10(np.linspace(0, 1, len(configs)))
    linestyles = ['-', '--', ':', '-.']
    
    for idx, (metric, label) in enumerate(zip(metrics, metric_labels)):
        ax = axes[idx]
        
        for cfg_idx, (_, config) in enumerate(configs.iterrows()):
            config_label = _config_label(config)
            color = colors[cfg_idx % len(colors)]
            
            for ws_idx, ws in enumerate(window_sizes):
                mask = (df['classifier'] == config['classifier']) & \
                       (df['ngram'] == config['ngram']) & \
                       (df['log_buckets'] == config['log_buckets']) & \
                       (df['num_hashes'] == config['num_hashes']) & \
                       (df['optimized_for'] == config['optimized_for']) & \
                       (df['window'] == ws)
                
                subset = df[mask].sort_values('num_examples')
                
                if len(subset) == 0:
                    continue
                
                # Only show legend for first window size
                show_label = f"{config_label} w={ws}" if ws == window_sizes[0] else None
                linestyle = linestyles[ws_idx % len(linestyles)]
                
                ax.plot(subset['num_examples'], subset[metric],
                       linestyle=linestyle, linewidth=1.5, alpha=0.7,
                       color=color, label=show_label if ws == window_sizes[0] else None)
        
        ax.set_xlabel('Number of Training Examples')
        ax.set_ylabel(label)
        ax.set_title(f'{label} Learning Curve')
        ax.grid(True, alpha=0.3)
        if idx == 0:
            ax.legend(loc='best', fontsize=7)
    
    # Hide unused subplot
    axes[-1].set_visible(False)
    
    plt.tight_layout()
    _save_png(fig, 'learning_curves_selected.png')
    print("  Saved: reports/learning_curves_selected.png")
    
    # === Matplotlib: Individual metric plots (all windows) ===
    for metric, label in zip(metrics, metric_labels):
        fig, ax = plt.subplots(figsize=(12, 7))
        
        for cfg_idx, (_, config) in enumerate(configs.iterrows()):
            config_label = _config_label(config)
            color = colors[cfg_idx % len(colors)]
            
            for ws_idx, ws in enumerate(window_sizes):
                mask = (df['classifier'] == config['classifier']) & \
                       (df['ngram'] == config['ngram']) & \
                       (df['log_buckets'] == config['log_buckets']) & \
                       (df['num_hashes'] == config['num_hashes']) & \
                       (df['optimized_for'] == config['optimized_for']) & \
                       (df['window'] == ws)
                
                subset = df[mask].sort_values('num_examples')
                
                if len(subset) == 0:
                    continue
                
                linestyle = linestyles[ws_idx % len(linestyles)]
                full_label = f"{config_label} w={ws}"
                
                ax.plot(subset['num_examples'], subset[metric],
                       linestyle=linestyle, linewidth=1.5, alpha=0.7,
                       color=color, label=full_label)
        
        ax.set_xlabel('Number of Training Examples')
        ax.set_ylabel(label)
        ax.set_title(f'{label} Learning Curves - All Configurations')
        ax.legend(loc='best', fontsize=7, ncol=2)
        ax.grid(True, alpha=0.3)
        
        plt.tight_layout()
        _save_png(fig, f'learning_curve_{metric}.png')
        print(f"  Saved: reports/learning_curve_{metric}.png")
    
    # === Plotly: Interactive with dropdown for metric selection ===
    fig_html = go.Figure()
    
    # Add traces for all metrics (will toggle visibility)
    for metric_idx, (metric, label) in enumerate(zip(metrics, metric_labels)):
        for cfg_idx, (_, config) in enumerate(configs.iterrows()):
            config_label = _config_label(config)
            
            for ws in window_sizes:
                mask = (df['classifier'] == config['classifier']) & \
                       (df['ngram'] == config['ngram']) & \
                       (df['log_buckets'] == config['log_buckets']) & \
                       (df['num_hashes'] == config['num_hashes']) & \
                       (df['optimized_for'] == config['optimized_for']) & \
                       (df['window'] == ws)
                
                subset = df[mask].sort_values('num_examples')
                
                if len(subset) == 0:
                    continue
                
                fig_html.add_trace(
                    go.Scatter(
                        x=subset['num_examples'],
                        y=subset[metric],
                        mode='lines+markers',
                        name=f"{config_label} w={ws}",
                        visible=(metric_idx == 4),  # F0.5 visible by default
                        legendgroup=f"{config_label} w={ws}",
                        showlegend=(metric_idx == 4),
                        hovertemplate=f'{label}: %{{y:.3f}}<br>Examples: %{{x}}'
                    )
                )
    
    # Calculate traces per metric
    traces_per_metric = len(configs) * len(window_sizes)
    
    # Create dropdown buttons
    buttons = []
    for metric_idx, (metric, label) in enumerate(zip(metrics, metric_labels)):
        visibility = [False] * (len(metrics) * traces_per_metric)
        for i in range(traces_per_metric):
            visibility[metric_idx * traces_per_metric + i] = True
        
        buttons.append(dict(
            label=label,
            method='update',
            args=[{'visible': visibility},
                  {'title': f'{label} Learning Curves - Selected Configurations'}]
        ))
    
    fig_html.update_layout(
        updatemenus=[dict(
            active=4,  # F0.5 default
            buttons=buttons,
            direction="down",
            x=0.1,
            y=1.15,
            showactive=True,
        )],
        title="F0.5 Learning Curves - Selected Configurations",
        xaxis_title="Number of Training Examples",
        yaxis_title="Score",
        height=600,
        width=1000
    )
    _save_html(fig_html, "learning_curves_selected.html")
    print("  Saved: reports/learning_curves_selected.html")


def plot_timing_4d():
    """
    Create 4D timing visualization: ngram (x), log_buckets (y), num_hashes (z), time (color).
    Uses 3D scatter plot with color representing time per email.
    """
    print("Plotting 4D timing visualization...")
    
    try:
        df = pd.read_csv('timing_results.csv')
    except FileNotFoundError:
        print("  Warning: timing_results.csv not found")
        return
    
    print(f"  Loaded {len(df)} timing data points")
    
    # Separate Feature Hashing (num_hashes=0) and Count-Min
    df_fh = df[df['num_hashes'] == 0].copy()
    df_cm = df[df['num_hashes'] > 0].copy()
    
    # === Plotly: Interactive 3D scatter ===
    fig_3d = go.Figure()
    
    # Feature Hashing (show as z=0 plane since no num_hashes)
    if len(df_fh) > 0:
        fig_3d.add_trace(
            go.Scatter3d(
                x=df_fh['ngram'],
                y=df_fh['log_buckets'],
                z=[0] * len(df_fh),  # z=0 for Feature Hashing
                mode='markers',
                marker=dict(
                    size=10,
                    color=df_fh['time_per_email_ms'],
                    colorscale='Blues',
                    colorbar=dict(title='Time (ms)', x=0.85),
                    showscale=True,
                ),
                name='Feature Hashing',
                text=[f"FH: ng={ng}, lb={lb}<br>Time: {t:.3f}ms" 
                      for ng, lb, t in zip(df_fh['ngram'], df_fh['log_buckets'], df_fh['time_per_email_ms'])],
                hoverinfo='text'
            )
        )
    
    # Count-Min
    if len(df_cm) > 0:
        fig_3d.add_trace(
            go.Scatter3d(
                x=df_cm['ngram'],
                y=df_cm['log_buckets'],
                z=df_cm['num_hashes'],
                mode='markers',
                marker=dict(
                    size=8,
                    color=df_cm['time_per_email_ms'],
                    colorscale='Greens',
                    colorbar=dict(title='Time (ms)', x=1.0),
                    showscale=True,
                ),
                name='Count-Min',
                text=[f"CM: ng={ng}, lb={lb}, nh={nh}<br>Time: {t:.3f}ms" 
                      for ng, lb, nh, t in zip(df_cm['ngram'], df_cm['log_buckets'], 
                                               df_cm['num_hashes'], df_cm['time_per_email_ms'])],
                hoverinfo='text'
            )
        )
    
    fig_3d.update_layout(
        title="Computational Efficiency: 4D Visualization<br>(ngram x log_buckets x num_hashes, color=time)",
        scene=dict(
            xaxis_title='N-gram Size',
            yaxis_title='Log Num Buckets',
            zaxis_title='Num Hashes (0=FH)',
        ),
        height=700,
        width=900,
    )
    _save_html(fig_3d, "timing_4d.html")
    print("  Saved: reports/timing_4d.html")
    
    # === Matplotlib: 2D heatmaps for each classifier ===
    fig, axes = plt.subplots(1, 2, figsize=(14, 5))
    
    # Feature Hashing heatmap
    ax = axes[0]
    if len(df_fh) > 0:
        pivot_fh = df_fh.pivot_table(values='time_per_email_ms', 
                                      index='ngram', columns='log_buckets')
        sns.heatmap(pivot_fh, annot=True, fmt='.3f', cmap='Blues', ax=ax,
                   cbar_kws={'label': 'Time per email (ms)'})
        ax.set_title('Feature Hashing: Time per Email')
        ax.set_xlabel('Log Num Buckets')
        ax.set_ylabel('N-gram Size')
    else:
        ax.text(0.5, 0.5, 'No Feature Hashing data', ha='center', va='center')
        ax.set_title('Feature Hashing')
    
    # Count-Min: average across num_hashes
    ax = axes[1]
    if len(df_cm) > 0:
        pivot_cm = df_cm.pivot_table(values='time_per_email_ms', 
                                      index='ngram', columns='log_buckets', aggfunc='mean')
        sns.heatmap(pivot_cm, annot=True, fmt='.3f', cmap='Greens', ax=ax,
                   cbar_kws={'label': 'Time per email (ms)'})
        ax.set_title('Count-Min: Avg Time per Email (across num_hashes)')
        ax.set_xlabel('Log Num Buckets')
        ax.set_ylabel('N-gram Size')
    else:
        ax.text(0.5, 0.5, 'No Count-Min data', ha='center', va='center')
        ax.set_title('Count-Min')
    
    plt.tight_layout()
    _save_png(fig, 'timing_heatmaps.png')
    print("  Saved: reports/timing_heatmaps.png")
    
    # === Bar chart comparison ===
    fig, ax = plt.subplots(figsize=(14, 6))
    
    # Create labels
    labels = []
    times = []
    colors = []
    
    for _, row in df.iterrows():
        if row['num_hashes'] == 0:
            labels.append(f"FH ng={int(row['ngram'])} lb={int(row['log_buckets'])}")
            colors.append('steelblue')
        else:
            labels.append(f"CM ng={int(row['ngram'])} lb={int(row['log_buckets'])} nh={int(row['num_hashes'])}")
            colors.append('forestgreen')
        times.append(row['time_per_email_ms'])
    
    x = np.arange(len(labels))
    ax.bar(x, times, color=colors, alpha=0.8)
    ax.set_xticks(x)
    ax.set_xticklabels(labels, rotation=90, fontsize=7)
    ax.set_ylabel('Time per Email (ms)')
    ax.set_title('Computational Efficiency: Time per Email by Configuration')
    ax.grid(True, alpha=0.3, axis='y')
    
    # Add legend
    from matplotlib.patches import Patch
    legend_elements = [Patch(facecolor='steelblue', alpha=0.8, label='Feature Hashing'),
                      Patch(facecolor='forestgreen', alpha=0.8, label='Count-Min')]
    ax.legend(handles=legend_elements)
    
    plt.tight_layout()
    _save_png(fig, 'timing_bar.png')
    print("  Saved: reports/timing_bar.png")


def plot_test_vs_dev():
    """
    Plot comparison of test vs dev performance for final configurations.
    Highlights potential overfitting when test << dev.
    """
    print("Plotting test vs dev comparison...")
    
    try:
        df = pd.read_csv('final_test_results.csv')
    except FileNotFoundError:
        print("  Warning: evalution_results.csv not found")
        return
    
    print(f"  Loaded {len(df)} final configurations")
    
    # Metrics to compare
    metric_pairs = [
        ('test_f05', 'dev_f05', 'F0.5'),
        ('test_precision', 'dev_precision', 'Precision'),
        ('test_recall', 'dev_recall', 'Recall'),
        ('test_accuracy', 'dev_accuracy', 'Accuracy'),
    ]
    
    # === Matplotlib: Grouped bar chart ===
    fig, axes = plt.subplots(2, 2, figsize=(14, 10))
    axes = axes.flatten()
    
    # Create configuration labels
    labels = [_config_label(row) for _, row in df.iterrows()]
    x = np.arange(len(labels))
    width = 0.35
    
    for idx, (test_col, dev_col, label) in enumerate(metric_pairs):
        ax = axes[idx]
        
        dev_vals = df[dev_col].values
        test_vals = df[test_col].values
        
        bars_dev = ax.bar(x - width/2, dev_vals, width, label='Dev Set', 
                         color='steelblue', alpha=0.8)
        bars_test = ax.bar(x + width/2, test_vals, width, label='Test Set', 
                          color='coral', alpha=0.8)
        
        # Highlight significant drops (overfitting)
        for i, (dev_v, test_v) in enumerate(zip(dev_vals, test_vals)):
            if test_v < dev_v - 0.03:  # More than 3% drop
                ax.annotate('!', xy=(x[i] + width/2, test_v), 
                           ha='center', va='bottom', fontsize=14, color='red', fontweight='bold')
        
        ax.set_xlabel('Configuration')
        ax.set_ylabel(label)
        ax.set_title(f'{label}: Dev vs Test Comparison')
        ax.set_xticks(x)
        ax.set_xticklabels(labels, rotation=45, ha='right', fontsize=7)
        ax.legend()
        ax.grid(True, alpha=0.3, axis='y')
    
    plt.tight_layout()
    _save_png(fig, 'test_vs_dev.png')
    print("  Saved: reports/test_vs_dev.png")
    
    # === Matplotlib: Scatter plot dev vs test ===
    fig, ax = plt.subplots(figsize=(10, 8))
    
    # Plot each metric with different markers
    markers = ['o', 's', '^', 'D']
    colors_m = ['blue', 'green', 'red', 'purple']
    
    for idx, (test_col, dev_col, label) in enumerate(metric_pairs):
        ax.scatter(df[dev_col], df[test_col], 
                  marker=markers[idx], s=100, alpha=0.7, 
                  color=colors_m[idx], label=label)
    
    # Add diagonal (perfect agreement)
    ax.plot([0.5, 1], [0.5, 1], 'k--', linewidth=1, alpha=0.5, label='Perfect agreement')
    
    ax.set_xlabel('Dev Set Score')
    ax.set_ylabel('Test Set Score')
    ax.set_title('Dev vs Test Performance (points below diagonal = overfitting)')
    ax.legend(loc='lower right')
    ax.grid(True, alpha=0.3)
    ax.set_xlim([0.5, 1.0])
    ax.set_ylim([0.5, 1.0])
    
    plt.tight_layout()
    _save_png(fig, 'test_vs_dev_scatter.png')
    print("  Saved: reports/test_vs_dev_scatter.png")
    
    # === Plotly: Interactive grouped bar ===
    fig_html = go.Figure()
    
    for test_col, dev_col, label in metric_pairs:
        fig_html.add_trace(go.Bar(
            name=f'{label} (Dev)',
            x=labels,
            y=df[dev_col],
            marker_color='steelblue',
            opacity=0.8
        ))
        fig_html.add_trace(go.Bar(
            name=f'{label} (Test)',
            x=labels,
            y=df[test_col],
            marker_color='coral',
            opacity=0.8
        ))
    
    fig_html.update_layout(
        barmode='group',
        title="Test vs Dev Comparison - All Metrics",
        xaxis_title="Configuration",
        yaxis_title="Score",
        height=600,
        width=1200,
        legend=dict(font=dict(size=9))
    )
    _save_html(fig_html, "test_vs_dev.html")
    print("  Saved: reports/test_vs_dev.html")
    
    # Print summary with warnings
    print("\n  === Test vs Dev Summary ===")
    for _, row in df.iterrows():
        config_label = _config_label(row)
        f05_diff = row['test_f05'] - row['dev_f05']
        
        print(f"  {config_label}:")
        print(f"    Dev:  F0.5={row['dev_f05']:.4f}, P={row['dev_precision']:.4f}, R={row['dev_recall']:.4f}")
        print(f"    Test: F0.5={row['test_f05']:.4f}, P={row['test_precision']:.4f}, R={row['test_recall']:.4f}")
        
        if f05_diff < -0.03:
            print(f"    WARNING: Test F0.5 is {-f05_diff*100:.1f}% lower - possible overfitting!")


def plot_config_summary():
    """
    Create summary visualization of final configurations.
    Shows F0.5 scores and parameter comparison.
    """
    print("Plotting configuration summary...")
    
    try:
        df = pd.read_csv('final_configs.csv')
    except FileNotFoundError:
        print("  Warning: final_configs.csv not found")
        return
    
    print(f"  Loaded {len(df)} final configurations")
    
    # === Matplotlib: Bar chart of F0.5 scores ===
    fig, ax = plt.subplots(figsize=(12, 6))
    
    labels = [_config_label(row, include_opt=True) for _, row in df.iterrows()]
    colors = ['steelblue' if row['classifier'] == 'FeatureHashing' else 'forestgreen' 
              for _, row in df.iterrows()]
    
    x = np.arange(len(labels))
    bars = ax.bar(x, df['f05'], color=colors, alpha=0.8)
    
    # Add value labels on bars
    for bar, val in zip(bars, df['f05']):
        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.005,
               f'{val:.3f}', ha='center', va='bottom', fontsize=9)
    
    ax.set_xticks(x)
    ax.set_xticklabels(labels, rotation=45, ha='right', fontsize=9)
    ax.set_ylabel('F0.5 Score')
    ax.set_title('Best F0.5 Scores by Configuration')
    ax.grid(True, alpha=0.3, axis='y')
    
    # Legend
    from matplotlib.patches import Patch
    legend_elements = [Patch(facecolor='steelblue', alpha=0.8, label='Feature Hashing'),
                      Patch(facecolor='forestgreen', alpha=0.8, label='Count-Min')]
    ax.legend(handles=legend_elements)
    
    plt.tight_layout()
    _save_png(fig, 'config_summary_f05.png')
    print("  Saved: reports/config_summary_f05.png")
    
    # === Matplotlib: Multi-metric comparison ===
    fig, ax = plt.subplots(figsize=(14, 7))
    
    metrics = ['f05', 'precision', 'recall', 'f1', 'accuracy']
    x = np.arange(len(labels))
    width = 0.15
    
    colors_metrics = plt.cm.Set2(np.linspace(0, 1, len(metrics)))
    
    for i, metric in enumerate(metrics):
        offset = (i - len(metrics)/2 + 0.5) * width
        ax.bar(x + offset, df[metric], width, label=metric.upper(), 
               color=colors_metrics[i], alpha=0.9)
    
    ax.set_xticks(x)
    ax.set_xticklabels(labels, rotation=45, ha='right', fontsize=8)
    ax.set_ylabel('Score')
    ax.set_title('All Metrics by Configuration')
    ax.legend(loc='upper right')
    ax.grid(True, alpha=0.3, axis='y')
    
    plt.tight_layout()
    _save_png(fig, 'config_summary_all_metrics.png')
    print("  Saved: reports/config_summary_all_metrics.png")
    
    # === Plotly: Interactive radar chart ===
    fig_radar = go.Figure()
    
    categories = ['F0.5', 'Precision', 'Recall', 'F1', 'Accuracy']
    
    for _, row in df.iterrows():
        label = _config_label(row)
        values = [row['f05'], row['precision'], row['recall'], row['f1'], row['accuracy']]
        values.append(values[0])  # Close the polygon
        
        fig_radar.add_trace(go.Scatterpolar(
            r=values,
            theta=categories + [categories[0]],
            fill='toself',
            name=label,
            opacity=0.6
        ))
    
    fig_radar.update_layout(
        polar=dict(radialaxis=dict(visible=True, range=[0.5, 1.0])),
        title="Configuration Comparison - Radar Chart",
        height=600,
        width=800
    )
    _save_html(fig_radar, "config_radar.html")
    print("  Saved: reports/config_radar.html")
    
    # === Summary table ===
    print("\n  === Configuration Summary ===")
    print(f"  {'Configuration':<45} {'F0.5':>8} {'Prec':>8} {'Recall':>8} {'Thresh':>8}")
    print("  " + "-" * 80)
    
    for _, row in df.iterrows():
        label = _config_label(row)
        print(f"  {label:<45} {row['f05']:>8.4f} {row['precision']:>8.4f} "
              f"{row['recall']:>8.4f} {row['best_threshold']:>8.3f}")
    
    # Find best overall
    best_idx = df['f05'].idxmax()
    best = df.loc[best_idx]
    print(f"\n  Best configuration: {_config_label(best)} with F0.5={best['f05']:.4f}")


def plot_cv_recall_at_precision():
    """
    Plot Recall@Precision=0.95 from cross-validation results.
    - Feature Hashing: 2D heatmap (ngram x log_buckets)
    - Count-Min: 3D visualization (ngram x log_buckets x num_hashes)
    """
    print("Plotting CV Recall@Precision=0.95...")
    
    try:
        df = pd.read_csv('cv_results.csv')
    except FileNotFoundError:
        print("  Warning: cv_results.csv not found")
        return
    
    print(f"  Loaded {len(df)} CV data points")
    
    # Average across folds
    agg_df = df.groupby(['classifier', 'ngram', 'log_buckets', 'num_hashes']).agg({
        'recall_at_prec_95': 'mean',
        'recall_at_prec_99': 'mean',
        'auc': 'mean',
        'recall_at_fpr_001': 'mean',
        'recall_at_fpr_0001': 'mean'
    }).reset_index()
    
    # === Feature Hashing: 2D Heatmap ===
    df_fh = agg_df[agg_df['classifier'] == 'FeatureHashing']
    
    if len(df_fh) > 0:
        fig, ax = plt.subplots(figsize=(10, 7))
        
        pivot_fh = df_fh.pivot_table(values='recall_at_prec_95', 
                                      index='ngram', columns='log_buckets')
        
        sns.heatmap(pivot_fh, annot=True, fmt='.3f', cmap='YlOrRd', ax=ax,
                   cbar_kws={'label': 'Recall @ Precision=0.95'},
                   vmin=0, vmax=1)
        ax.set_title('Feature Hashing: Recall @ Precision=0.95 (CV Average)')
        ax.set_xlabel('Log Num Buckets')
        ax.set_ylabel('N-gram Size')
        
        plt.tight_layout()
        _save_png(fig, 'cv_recall_at_prec95_fh.png')
        print("  Saved: reports/cv_recall_at_prec95_fh.png")
    
    # === Count-Min: 3D Visualization ===
    df_cm = agg_df[agg_df['classifier'] == 'CountMin']
    
    if len(df_cm) > 0:
        # 3D scatter plot with color = recall_at_prec_95
        fig_3d = go.Figure()
        
        fig_3d.add_trace(
            go.Scatter3d(
                x=df_cm['ngram'],
                y=df_cm['log_buckets'],
                z=df_cm['num_hashes'],
                mode='markers',
                marker=dict(
                    size=10,
                    color=df_cm['recall_at_prec_95'],
                    colorscale='YlOrRd',
                    colorbar=dict(title='Recall@P=0.95'),
                    showscale=True,
                    cmin=0,
                    cmax=1,
                ),
                text=[f"ng={ng}, lb={lb}, nh={nh}<br>Rec@P95: {r:.3f}" 
                      for ng, lb, nh, r in zip(df_cm['ngram'], df_cm['log_buckets'], 
                                               df_cm['num_hashes'], df_cm['recall_at_prec_95'])],
                hoverinfo='text'
            )
        )
        
        fig_3d.update_layout(
            title="Count-Min: Recall @ Precision=0.95 (CV Average)",
            scene=dict(
                xaxis_title='N-gram Size',
                yaxis_title='Log Num Buckets',
                zaxis_title='Num Hashes',
            ),
            height=700,
            width=900,
        )
        _save_html(fig_3d, "cv_recall_at_prec95_cm_3d.html")
        print("  Saved: reports/cv_recall_at_prec95_cm_3d.html")
        
        # Also create 2D heatmaps for each num_hashes value
        num_hashes_vals = sorted(df_cm['num_hashes'].unique())
        
        fig, axes = plt.subplots(1, len(num_hashes_vals), figsize=(5*len(num_hashes_vals), 5))
        if len(num_hashes_vals) == 1:
            axes = [axes]
        
        for i, nh in enumerate(num_hashes_vals):
            ax = axes[i]
            df_subset = df_cm[df_cm['num_hashes'] == nh]
            pivot = df_subset.pivot_table(values='recall_at_prec_95', 
                                          index='ngram', columns='log_buckets')
            
            sns.heatmap(pivot, annot=True, fmt='.3f', cmap='YlOrRd', ax=ax,
                       cbar_kws={'label': 'Recall@P=0.95'},
                       vmin=0, vmax=1)
            ax.set_title(f'Count-Min (nh={int(nh)})')
            ax.set_xlabel('Log Num Buckets')
            ax.set_ylabel('N-gram Size')
        
        plt.tight_layout()
        _save_png(fig, 'cv_recall_at_prec95_cm.png')
        print("  Saved: reports/cv_recall_at_prec95_cm.png")
    
    # === Combined interactive heatmap ===
    fig_html = make_subplots(
        rows=1, cols=2,
        subplot_titles=["Feature Hashing", "Count-Min (avg across num_hashes)"]
    )
    
    if len(df_fh) > 0:
        pivot_fh = df_fh.pivot_table(values='recall_at_prec_95', 
                                      index='ngram', columns='log_buckets')
        fig_html.add_trace(
            go.Heatmap(
                z=pivot_fh.values,
                x=pivot_fh.columns.astype(str),
                y=pivot_fh.index.astype(str),
                colorscale='YlOrRd',
                zmin=0, zmax=1,
                colorbar=dict(title='Rec@P95', x=0.45),
                showscale=True,
                text=[[f'{v:.3f}' for v in row] for row in pivot_fh.values],
                texttemplate='%{text}',
                hovertemplate='ngram=%{y}<br>log_buckets=%{x}<br>Rec@P95=%{z:.3f}<extra></extra>'
            ),
            row=1, col=1
        )
    
    if len(df_cm) > 0:
        pivot_cm = df_cm.pivot_table(values='recall_at_prec_95', 
                                      index='ngram', columns='log_buckets', aggfunc='mean')
        fig_html.add_trace(
            go.Heatmap(
                z=pivot_cm.values,
                x=pivot_cm.columns.astype(str),
                y=pivot_cm.index.astype(str),
                colorscale='YlOrRd',
                zmin=0, zmax=1,
                colorbar=dict(title='Rec@P95', x=1.0),
                showscale=True,
                text=[[f'{v:.3f}' for v in row] for row in pivot_cm.values],
                texttemplate='%{text}',
                hovertemplate='ngram=%{y}<br>log_buckets=%{x}<br>Rec@P95=%{z:.3f}<extra></extra>'
            ),
            row=1, col=2
        )
    
    fig_html.update_xaxes(title_text="Log Num Buckets", row=1, col=1)
    fig_html.update_yaxes(title_text="N-gram Size", row=1, col=1)
    fig_html.update_xaxes(title_text="Log Num Buckets", row=1, col=2)
    fig_html.update_yaxes(title_text="N-gram Size", row=1, col=2)
    fig_html.update_layout(
        title="CV Recall @ Precision=0.95",
        height=500,
        width=1000
    )
    _save_html(fig_html, "cv_recall_at_prec95.html")
    print("  Saved: reports/cv_recall_at_prec95.html")


def plot_timing_by_dimension():
    """
    Plot timing by each hyperparameter dimension.
    Shows average time vs ngram, log_buckets, and num_hashes separately.
    """
    print("Plotting timing by dimension...")
    
    try:
        df = pd.read_csv('timing_results.csv')
    except FileNotFoundError:
        print("  Warning: timing_results.csv not found")
        return
    
    print(f"  Loaded {len(df)} timing data points")
    
    # Separate classifiers
    df_fh = df[df['num_hashes'] == 0].copy()
    df_cm = df[df['num_hashes'] > 0].copy()
    
    # === Matplotlib: 3 subplots for each dimension ===
    fig, axes = plt.subplots(1, 3, figsize=(16, 5))
    
    # --- Plot 1: Time vs N-gram ---
    ax = axes[0]
    
    # Feature Hashing: average across log_buckets
    if len(df_fh) > 0:
        fh_by_ngram = df_fh.groupby('ngram')['time_per_email_ms'].mean()
        ax.plot(fh_by_ngram.index, fh_by_ngram.values, 'o-', 
               linewidth=2, markersize=8, color='steelblue', label='Feature Hashing')
    
    # Count-Min: average across log_buckets and num_hashes
    if len(df_cm) > 0:
        cm_by_ngram = df_cm.groupby('ngram')['time_per_email_ms'].mean()
        ax.plot(cm_by_ngram.index, cm_by_ngram.values, 's--', 
               linewidth=2, markersize=8, color='forestgreen', label='Count-Min')
    
    ax.set_xlabel('N-gram Size')
    ax.set_ylabel('Avg Time per Email (ms)')
    ax.set_title('Time vs N-gram Size')
    ax.legend()
    ax.grid(True, alpha=0.3)
    ax.set_xticks(sorted(df['ngram'].unique()))
    
    # --- Plot 2: Time vs Log Buckets ---
    ax = axes[1]
    
    if len(df_fh) > 0:
        fh_by_lb = df_fh.groupby('log_buckets')['time_per_email_ms'].mean()
        ax.plot(fh_by_lb.index, fh_by_lb.values, 'o-', 
               linewidth=2, markersize=8, color='steelblue', label='Feature Hashing')
    
    if len(df_cm) > 0:
        cm_by_lb = df_cm.groupby('log_buckets')['time_per_email_ms'].mean()
        ax.plot(cm_by_lb.index, cm_by_lb.values, 's--', 
               linewidth=2, markersize=8, color='forestgreen', label='Count-Min')
    
    ax.set_xlabel('Log Num Buckets')
    ax.set_ylabel('Avg Time per Email (ms)')
    ax.set_title('Time vs Log Num Buckets')
    ax.legend()
    ax.grid(True, alpha=0.3)
    ax.set_xticks(sorted(df['log_buckets'].unique()))
    
    # --- Plot 3: Time vs Num Hashes (Count-Min only) ---
    ax = axes[2]
    
    if len(df_cm) > 0:
        cm_by_nh = df_cm.groupby('num_hashes')['time_per_email_ms'].mean()
        ax.plot(cm_by_nh.index, cm_by_nh.values, 's-', 
               linewidth=2, markersize=8, color='forestgreen', label='Count-Min')
        ax.set_xticks(sorted(df_cm['num_hashes'].unique()))
    else:
        ax.text(0.5, 0.5, 'No Count-Min data', ha='center', va='center', transform=ax.transAxes)
    
    ax.set_xlabel('Num Hashes')
    ax.set_ylabel('Avg Time per Email (ms)')
    ax.set_title('Time vs Num Hashes (Count-Min only)')
    if len(df_cm) > 0:
        ax.legend()
    ax.grid(True, alpha=0.3)
    
    plt.tight_layout()
    _save_png(fig, 'timing_by_dimension.png')
    print("  Saved: reports/timing_by_dimension.png")
    
    # === Bar chart version ===
    fig, axes = plt.subplots(1, 3, figsize=(16, 5))
    
    # N-gram bars
    ax = axes[0]
    ngrams = sorted(df['ngram'].unique())
    x = np.arange(len(ngrams))
    width = 0.35
    
    if len(df_fh) > 0:
        fh_vals = [df_fh[df_fh['ngram'] == ng]['time_per_email_ms'].mean() for ng in ngrams]
        ax.bar(x - width/2, fh_vals, width, label='Feature Hashing', color='steelblue', alpha=0.8)
    
    if len(df_cm) > 0:
        cm_vals = [df_cm[df_cm['ngram'] == ng]['time_per_email_ms'].mean() for ng in ngrams]
        ax.bar(x + width/2, cm_vals, width, label='Count-Min', color='forestgreen', alpha=0.8)
    
    ax.set_xlabel('N-gram Size')
    ax.set_ylabel('Avg Time per Email (ms)')
    ax.set_title('Time vs N-gram Size')
    ax.set_xticks(x)
    ax.set_xticklabels(ngrams)
    ax.legend()
    ax.grid(True, alpha=0.3, axis='y')
    
    # Log buckets bars
    ax = axes[1]
    log_buckets = sorted(df['log_buckets'].unique())
    x = np.arange(len(log_buckets))
    
    if len(df_fh) > 0:
        fh_vals = [df_fh[df_fh['log_buckets'] == lb]['time_per_email_ms'].mean() for lb in log_buckets]
        ax.bar(x - width/2, fh_vals, width, label='Feature Hashing', color='steelblue', alpha=0.8)
    
    if len(df_cm) > 0:
        cm_vals = [df_cm[df_cm['log_buckets'] == lb]['time_per_email_ms'].mean() for lb in log_buckets]
        ax.bar(x + width/2, cm_vals, width, label='Count-Min', color='forestgreen', alpha=0.8)
    
    ax.set_xlabel('Log Num Buckets')
    ax.set_ylabel('Avg Time per Email (ms)')
    ax.set_title('Time vs Log Num Buckets')
    ax.set_xticks(x)
    ax.set_xticklabels(log_buckets)
    ax.legend()
    ax.grid(True, alpha=0.3, axis='y')
    
    # Num hashes bars (Count-Min only)
    ax = axes[2]
    if len(df_cm) > 0:
        num_hashes = sorted(df_cm['num_hashes'].unique())
        x = np.arange(len(num_hashes))
        cm_vals = [df_cm[df_cm['num_hashes'] == nh]['time_per_email_ms'].mean() for nh in num_hashes]
        ax.bar(x, cm_vals, 0.6, label='Count-Min', color='forestgreen', alpha=0.8)
        ax.set_xticks(x)
        ax.set_xticklabels([int(nh) for nh in num_hashes])
        ax.legend()
    else:
        ax.text(0.5, 0.5, 'No Count-Min data', ha='center', va='center', transform=ax.transAxes)
    
    ax.set_xlabel('Num Hashes')
    ax.set_ylabel('Avg Time per Email (ms)')
    ax.set_title('Time vs Num Hashes (Count-Min only)')
    ax.grid(True, alpha=0.3, axis='y')
    
    plt.tight_layout()
    _save_png(fig, 'timing_by_dimension_bars.png')
    print("  Saved: reports/timing_by_dimension_bars.png")
    
    # === Plotly interactive ===
    fig_html = make_subplots(
        rows=1, cols=3,
        subplot_titles=["Time vs N-gram", "Time vs Log Buckets", "Time vs Num Hashes"]
    )
    
    # N-gram
    if len(df_fh) > 0:
        fh_by_ngram = df_fh.groupby('ngram')['time_per_email_ms'].mean().reset_index()
        fig_html.add_trace(
            go.Scatter(x=fh_by_ngram['ngram'], y=fh_by_ngram['time_per_email_ms'],
                      mode='lines+markers', name='Feature Hashing',
                      line=dict(color='steelblue'), marker=dict(size=10)),
            row=1, col=1
        )
    
    if len(df_cm) > 0:
        cm_by_ngram = df_cm.groupby('ngram')['time_per_email_ms'].mean().reset_index()
        fig_html.add_trace(
            go.Scatter(x=cm_by_ngram['ngram'], y=cm_by_ngram['time_per_email_ms'],
                      mode='lines+markers', name='Count-Min',
                      line=dict(color='forestgreen', dash='dash'), marker=dict(size=10, symbol='square')),
            row=1, col=1
        )
    
    # Log buckets
    if len(df_fh) > 0:
        fh_by_lb = df_fh.groupby('log_buckets')['time_per_email_ms'].mean().reset_index()
        fig_html.add_trace(
            go.Scatter(x=fh_by_lb['log_buckets'], y=fh_by_lb['time_per_email_ms'],
                      mode='lines+markers', name='Feature Hashing', showlegend=False,
                      line=dict(color='steelblue'), marker=dict(size=10)),
            row=1, col=2
        )
    
    if len(df_cm) > 0:
        cm_by_lb = df_cm.groupby('log_buckets')['time_per_email_ms'].mean().reset_index()
        fig_html.add_trace(
            go.Scatter(x=cm_by_lb['log_buckets'], y=cm_by_lb['time_per_email_ms'],
                      mode='lines+markers', name='Count-Min', showlegend=False,
                      line=dict(color='forestgreen', dash='dash'), marker=dict(size=10, symbol='square')),
            row=1, col=2
        )
    
    # Num hashes
    if len(df_cm) > 0:
        cm_by_nh = df_cm.groupby('num_hashes')['time_per_email_ms'].mean().reset_index()
        fig_html.add_trace(
            go.Scatter(x=cm_by_nh['num_hashes'], y=cm_by_nh['time_per_email_ms'],
                      mode='lines+markers', name='Count-Min', showlegend=False,
                      line=dict(color='forestgreen'), marker=dict(size=10, symbol='square')),
            row=1, col=3
        )
    
    fig_html.update_xaxes(title_text="N-gram Size", row=1, col=1)
    fig_html.update_xaxes(title_text="Log Num Buckets", row=1, col=2)
    fig_html.update_xaxes(title_text="Num Hashes", row=1, col=3)
    fig_html.update_yaxes(title_text="Avg Time (ms)", row=1, col=1)
    fig_html.update_yaxes(title_text="Avg Time (ms)", row=1, col=2)
    fig_html.update_yaxes(title_text="Avg Time (ms)", row=1, col=3)
    
    fig_html.update_layout(
        title="Timing by Hyperparameter Dimension",
        height=450,
        width=1200
    )
    _save_html(fig_html, "timing_by_dimension.html")
    print("  Saved: reports/timing_by_dimension.html")
    
    # Print summary
    print("\n  === Timing Summary by Dimension ===")
    
    print("\n  By N-gram:")
    for ng in sorted(df['ngram'].unique()):
        fh_time = df_fh[df_fh['ngram'] == ng]['time_per_email_ms'].mean() if len(df_fh) > 0 else float('nan')
        cm_time = df_cm[df_cm['ngram'] == ng]['time_per_email_ms'].mean() if len(df_cm) > 0 else float('nan')
        print(f"    ng={int(ng)}: FH={fh_time:.3f}ms, CM={cm_time:.3f}ms")
    
    print("\n  By Log Buckets:")
    for lb in sorted(df['log_buckets'].unique()):
        fh_time = df_fh[df_fh['log_buckets'] == lb]['time_per_email_ms'].mean() if len(df_fh) > 0 else float('nan')
        cm_time = df_cm[df_cm['log_buckets'] == lb]['time_per_email_ms'].mean() if len(df_cm) > 0 else float('nan')
        print(f"    lb={int(lb)}: FH={fh_time:.3f}ms, CM={cm_time:.3f}ms")
    
    if len(df_cm) > 0:
        print("\n  By Num Hashes (Count-Min only):")
        for nh in sorted(df_cm['num_hashes'].unique()):
            cm_time = df_cm[df_cm['num_hashes'] == nh]['time_per_email_ms'].mean()
            print(f"    nh={int(nh)}: CM={cm_time:.3f}ms")


def plot_space_usage():
    """
    Plot theoretical space usage vs log_buckets.
    - Feature Hashing: 2 arrays (spam/ham) x 2^log_buckets x 8 bytes
    - Count-Min: 2 arrays (spam/ham) x num_hashes x 2^log_buckets x 8 bytes
    """
    print("Plotting space usage...")
    
    # Parameters
    log_buckets_range = np.arange(8, 22)  # 8 to 21
    num_hashes_options = [2, 3, 4, 5]
    bytes_per_counter = 8  # double precision
    
    # Calculate space for Feature Hashing
    # 2 arrays (spam counts, ham counts) x 2^log_buckets counters x 8 bytes
    fh_space = 2 * (2 ** log_buckets_range) * bytes_per_counter
    
    # Calculate space for Count-Min (for each num_hashes)
    # 2 arrays x num_hashes rows x 2^log_buckets counters x 8 bytes
    cm_space = {}
    for nh in num_hashes_options:
        cm_space[nh] = 2 * nh * (2 ** log_buckets_range) * bytes_per_counter
    
    # === Matplotlib: Line plot ===
    fig, ax = plt.subplots(figsize=(12, 7))
    
    # Feature Hashing
    ax.plot(log_buckets_range, fh_space / 1024, 'o-', linewidth=2, markersize=6,
           color='steelblue', label='Feature Hashing')
    
    # Count-Min for different num_hashes
    cm_colors = plt.cm.Greens(np.linspace(0.4, 0.9, len(num_hashes_options)))
    for i, nh in enumerate(num_hashes_options):
        ax.plot(log_buckets_range, cm_space[nh] / 1024, 's--', linewidth=2, markersize=5,
               color=cm_colors[i], label=f'Count-Min (nh={nh})')
    
    ax.set_xlabel('Log Num Buckets')
    ax.set_ylabel('Space Usage (KB)')
    ax.set_title('Memory Usage vs Log Num Buckets')
    ax.legend()
    ax.grid(True, alpha=0.3)
    ax.set_xticks(log_buckets_range)
    
    # Add secondary y-axis in MB for large values
    ax2 = ax.secondary_yaxis('right', functions=(lambda x: x/1024, lambda x: x*1024))
    ax2.set_ylabel('Space Usage (MB)')
    
    plt.tight_layout()
    _save_png(fig, 'space_usage.png')
    print("  Saved: reports/space_usage.png")
    
    # === Log scale version ===
    fig, ax = plt.subplots(figsize=(12, 7))
    
    ax.semilogy(log_buckets_range, fh_space, 'o-', linewidth=2, markersize=6,
               color='steelblue', label='Feature Hashing')
    
    for i, nh in enumerate(num_hashes_options):
        ax.semilogy(log_buckets_range, cm_space[nh], 's--', linewidth=2, markersize=5,
                   color=cm_colors[i], label=f'Count-Min (nh={nh})')
    
    ax.set_xlabel('Log Num Buckets')
    ax.set_ylabel('Space Usage (bytes, log scale)')
    ax.set_title('Memory Usage vs Log Num Buckets (Log Scale)')
    ax.legend()
    ax.grid(True, alpha=0.3, which='both')
    ax.set_xticks(log_buckets_range)
    
    # Add reference lines for common memory sizes
    memory_refs = [(1024, '1 KB'), (1024**2, '1 MB'), (1024**3, '1 GB')]
    for size, label in memory_refs:
        if fh_space.min() < size < cm_space[max(num_hashes_options)].max():
            ax.axhline(y=size, color='red', linestyle=':', alpha=0.5, linewidth=1)
            ax.text(log_buckets_range[-1] + 0.3, size, label, va='center', fontsize=9, color='red')
    
    plt.tight_layout()
    _save_png(fig, 'space_usage_log.png')
    print("  Saved: reports/space_usage_log.png")
    
    # === Plotly interactive ===
    fig_html = go.Figure()
    
    fig_html.add_trace(
        go.Scatter(x=log_buckets_range, y=fh_space,
                  mode='lines+markers', name='Feature Hashing',
                  line=dict(color='steelblue'),
                  hovertemplate='log_buckets=%{x}<br>Space: %{y:,.0f} bytes<br>(%{customdata:.2f} KB)',
                  customdata=fh_space/1024)
    )
    
    for i, nh in enumerate(num_hashes_options):
        fig_html.add_trace(
            go.Scatter(x=log_buckets_range, y=cm_space[nh],
                      mode='lines+markers', name=f'Count-Min (nh={nh})',
                      line=dict(dash='dash'),
                      hovertemplate=f'log_buckets=%{{x}}<br>nh={nh}<br>Space: %{{y:,.0f}} bytes<br>(%{{customdata:.2f}} KB)',
                      customdata=cm_space[nh]/1024)
        )
    
    fig_html.update_layout(
        title="Memory Usage vs Log Num Buckets",
        xaxis_title="Log Num Buckets",
        yaxis_title="Space Usage (bytes)",
        yaxis_type="log",
        height=600,
        width=900
    )
    
    # Add reference lines
    for size, label in memory_refs:
        fig_html.add_hline(y=size, line_dash="dot", line_color="red", opacity=0.5,
                          annotation_text=label, annotation_position="right")
    
    _save_html(fig_html, "space_usage.html")
    print("  Saved: reports/space_usage.html")
    
    # Print summary table
    print("\n  === Space Usage Summary ===")
    print(f"  {'log_buckets':<12} {'FH (KB)':<12} " + " ".join([f'CM nh={nh} (KB)' for nh in num_hashes_options]))
    print("  " + "-" * 70)
    for i, lb in enumerate(log_buckets_range):
        fh_kb = fh_space[i] / 1024
        cm_kbs = [cm_space[nh][i] / 1024 for nh in num_hashes_options]
        print(f"  {lb:<12} {fh_kb:<12.1f} " + " ".join([f'{kb:<12.1f}' for kb in cm_kbs]))


def main():
    """
    Run all visualization functions for selected configurations.
    """
    print("=" * 60)
    print("Spam Filter Visualization")
    print("=" * 60)
    
    # Create output directory
    OUTPUT_DIR.mkdir(exist_ok=True)
    
    # Cross-validation plots
    plot_cv_recall_at_precision()
    print()
    
    # Space and timing analysis
    plot_space_usage()
    print()
    
    plot_timing_4d()
    print()
    
    plot_timing_by_dimension()
    print()
    
    # Selected configuration plots
    plot_pr_curves_selected()
    print()
    
    plot_roc_curves_selected()
    print()
    
    plot_learning_curves_selected()
    print()
    
    plot_test_vs_dev()
    print()
    
    plot_config_summary()
    
    print("\n" + "=" * 60)
    print("All visualizations complete!")
    print(f"Output saved to: {OUTPUT_DIR.absolute()}")
    print("=" * 60)


if __name__ == '__main__':
    main()
