#!/usr/bin/env python3
"""
Visualization script for spam filter experiments.
Reads CSV files generated by the C++ experiments and creates plots.
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
from pathlib import Path

# Set style for better-looking plots
sns.set_style("whitegrid")
sns.set_palette("husl")
plt.rcParams['figure.figsize'] = (12, 8)
plt.rcParams['font.size'] = 10


def plot_learning_curves():
    """
    Plot learning curves showing how metrics evolve with training examples.
    Experiment 1: Compare Feature Hashing vs Count-Min across different window sizes.
    """
    print("Plotting learning curves...")
    
    try:
        df = pd.read_csv('learning_curves.csv')
    except FileNotFoundError:
        print("Warning: learning_curves.csv not found")
        return
    
    # Create subplots for each metric
    metrics = ['accuracy', 'precision', 'recall', 'f1', 'fpr', 'fnr']
    fig, axes = plt.subplots(2, 3, figsize=(18, 10))
    axes = axes.flatten()
    
    for idx, metric in enumerate(metrics):
        ax = axes[idx]
        
        # Plot each classifier and window size combination
        for classifier in df['classifier'].unique():
            for window in df['window'].unique():
                subset = df[(df['classifier'] == classifier) & (df['window'] == window)]
                
                if len(subset) > 0:
                    label = f"{classifier[:2]}-w{window}"
                    ax.plot(subset['num_examples'], subset[metric], 
                           marker='o', markersize=3, label=label, alpha=0.7)
        
        ax.set_xlabel('Number of Training Examples')
        ax.set_ylabel(metric.upper())
        ax.set_title(f'{metric.upper()} vs Training Size')
        ax.legend(loc='best', fontsize=8)
        ax.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('learning_curves.png', dpi=300, bbox_inches='tight')
    print("Saved: learning_curves.png")
    plt.close()
    
    # Create a focused plot comparing just the two classifiers with optimal window
    fig, axes = plt.subplots(2, 2, figsize=(14, 10))
    
    # Focus on window=200 for cleaner comparison
    df_focused = df[df['window'] == 200]
    
    key_metrics = ['accuracy', 'precision', 'recall', 'f1']
    axes_flat = [axes[0, 0], axes[0, 1], axes[1, 0], axes[1, 1]]
    
    for ax, metric in zip(axes_flat, key_metrics):
        for classifier in df_focused['classifier'].unique():
            subset = df_focused[df_focused['classifier'] == classifier]
            ax.plot(subset['num_examples'], subset[metric], 
                   marker='o', markersize=4, label=classifier, linewidth=2)
        
        ax.set_xlabel('Number of Training Examples')
        ax.set_ylabel(metric.upper())
        ax.set_title(f'{metric.upper()} Learning Curve (window=200)')
        ax.legend()
        ax.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('learning_curves_focused.png', dpi=300, bbox_inches='tight')
    print("Saved: learning_curves_focused.png")
    plt.close()
    
    # Create all-in-one plot with all metrics on a single graph
    print("Creating all-in-one learning curve plot...")
    fig, ax = plt.subplots(1, 1, figsize=(14, 8))
    
    # Use window=200 for cleaner visualization
    df_w200 = df[df['window'] == 200]
    
    # Color map for classifiers
    colors = {'FeatureHashing': 'blue', 'CountMin': 'green'}
    
    for metric in metrics:
        for classifier in df_w200['classifier'].unique():
            subset = df_w200[df_w200['classifier'] == classifier]
            
            # Use different line styles for different metrics
            linestyle = '--' if metric in ['fpr', 'fnr'] else '-'
            alpha = 0.6 if metric in ['fpr', 'fnr'] else 0.8
            
            label = f"{classifier[:2]}-{metric}"
            ax.plot(subset['num_examples'], subset[metric], 
                   label=label, linewidth=2, alpha=alpha, 
                   linestyle=linestyle, color=colors.get(classifier, 'black'))
    
    ax.set_xlabel('Number of Training Examples')
    ax.set_ylabel('Metric Value')
    ax.set_title('All Metrics Learning Curves (window=200)')
    ax.legend(loc='best', fontsize=8, ncol=2)
    ax.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('learning_curves_all_in_one.png', dpi=300, bbox_inches='tight')
    print("Saved: learning_curves_all_in_one.png")
    plt.close()
    
    # Create individual plots for each metric
    print("Creating individual metric learning curve plots...")
    for metric in metrics:
        fig, ax = plt.subplots(1, 1, figsize=(10, 6))
        
        # Plot all classifiers and all window sizes for this metric
        for classifier in df['classifier'].unique():
            for window in df['window'].unique():
                subset = df[(df['classifier'] == classifier) & (df['window'] == window)]
                
                if len(subset) > 0:
                    label = f"{classifier[:2]}-w{window}"
                    ax.plot(subset['num_examples'], subset[metric], 
                           marker='o', markersize=3, label=label, alpha=0.7, linewidth=2)
        
        ax.set_xlabel('Number of Training Examples')
        ax.set_ylabel(metric.upper())
        ax.set_title(f'{metric.upper()} Learning Curve - All Configurations')
        ax.legend(loc='best', fontsize=9)
        ax.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(f'learning_curve_{metric}.png', dpi=300, bbox_inches='tight')
        print(f"Saved: learning_curve_{metric}.png")
        plt.close()


def plot_hyperparameter_heatmaps():
    """
    Visualize hyperparameter impacts using heatmaps.
    Experiment 4: Show which configurations work best.
    """
    print("Plotting hyperparameter heatmaps...")
    
    try:
        df = pd.read_csv('hyperparameter_results.csv')
    except FileNotFoundError:
        print("Warning: hyperparameter_results.csv not found")
        return
    
    # Filter to keep only final step for each configuration
    df = df.groupby(['classifier', 'ngram', 'log_buckets', 'num_hashes']).apply(
        lambda x: x.loc[x['num_examples'].idxmax()]
    ).reset_index(drop=True)
    
    # Feature Hashing heatmaps
    df_fh = df[df['classifier'] == 'FeatureHashing']
    
    fig, axes = plt.subplots(2, 2, figsize=(14, 10))
    
    metrics = ['precision', 'recall', 'f1', 'accuracy']
    
    for idx, metric in enumerate(metrics):
        ax = axes[idx // 2, idx % 2]
        
        # Pivot table for heatmap
        pivot = df_fh.pivot_table(values=metric, 
                                   index='ngram', 
                                   columns='log_buckets',
                                   aggfunc='mean')
        
        sns.heatmap(pivot, annot=True, fmt='.3f', cmap='YlGnBu', ax=ax,
                   cbar_kws={'label': metric.upper()})
        ax.set_title(f'Feature Hashing: {metric.upper()}')
        ax.set_xlabel('Log Num Buckets')
        ax.set_ylabel('N-gram Size')
    
    plt.tight_layout()
    plt.savefig('hyperparameter_heatmap_fh.png', dpi=300, bbox_inches='tight')
    print("Saved: hyperparameter_heatmap_fh.png")
    plt.close()
    
    # Count-Min heatmaps (more complex due to 3 parameters)
    df_cm = df[df['classifier'] == 'CountMin']
    
    # Create separate heatmaps for each num_hashes value
    num_hashes_values = sorted(df_cm['num_hashes'].unique())
    
    fig, axes = plt.subplots(len(num_hashes_values), 2, 
                            figsize=(12, 4*len(num_hashes_values)))
    
    for i, nh in enumerate(num_hashes_values):
        df_subset = df_cm[df_cm['num_hashes'] == nh]
        
        # Precision heatmap
        ax = axes[i, 0]
        pivot = df_subset.pivot_table(values='precision',
                                       index='ngram',
                                       columns='log_buckets',
                                       aggfunc='mean')
        sns.heatmap(pivot, annot=True, fmt='.3f', cmap='YlGnBu', ax=ax,
                   cbar_kws={'label': 'Precision'})
        ax.set_title(f'Count-Min (num_hashes={nh}): Precision')
        ax.set_xlabel('Log Num Buckets')
        ax.set_ylabel('N-gram Size')
        
        # F1 heatmap
        ax = axes[i, 1]
        pivot = df_subset.pivot_table(values='f1',
                                       index='ngram',
                                       columns='log_buckets',
                                       aggfunc='mean')
        sns.heatmap(pivot, annot=True, fmt='.3f', cmap='YlGnBu', ax=ax,
                   cbar_kws={'label': 'F1 Score'})
        ax.set_title(f'Count-Min (num_hashes={nh}): F1')
        ax.set_xlabel('Log Num Buckets')
        ax.set_ylabel('N-gram Size')
    
    plt.tight_layout()
    plt.savefig('hyperparameter_heatmap_cm.png', dpi=300, bbox_inches='tight')
    print("Saved: hyperparameter_heatmap_cm.png")
    plt.close()


def plot_precision_recall_tradeoff():
    """
    Plot precision-recall trade-offs for different configurations.
    Shows the balance between catching spam and avoiding false positives.
    """
    print("Plotting precision-recall trade-off...")
    
    try:
        df = pd.read_csv('hyperparameter_results.csv')
    except FileNotFoundError:
        print("Warning: hyperparameter_results.csv not found")
        return
    
    # Filter to keep only final step for each configuration
    df = df.groupby(['classifier', 'ngram', 'log_buckets', 'num_hashes']).apply(
        lambda x: x.loc[x['num_examples'].idxmax()]
    ).reset_index(drop=True)
    
    fig, axes = plt.subplots(1, 2, figsize=(14, 6))
    
    # Feature Hashing
    ax = axes[0]
    df_fh = df[df['classifier'] == 'FeatureHashing']
    
    for ngram in sorted(df_fh['ngram'].unique()):
        subset = df_fh[df_fh['ngram'] == ngram]
        ax.scatter(subset['recall'], subset['precision'], 
                  s=100, alpha=0.7, label=f'ngram={ngram}')
        
        # Add annotations for log_buckets
        for _, row in subset.iterrows():
            ax.annotate(f"{int(row['log_buckets'])}", 
                       (row['recall'], row['precision']),
                       fontsize=8, alpha=0.6)
    
    ax.set_xlabel('Recall (Spam Detection Rate)')
    ax.set_ylabel('Precision (Spam Prediction Accuracy)')
    ax.set_title('Feature Hashing: Precision-Recall Trade-off')
    ax.legend()
    ax.grid(True, alpha=0.3)
    ax.set_xlim([0.8, 1.0])
    ax.set_ylim([0.8, 1.0])
    
    # Count-Min
    ax = axes[1]
    df_cm = df[df['classifier'] == 'CountMin']
    
    for ngram in sorted(df_cm['ngram'].unique()):
        subset = df_cm[df_cm['ngram'] == ngram]
        ax.scatter(subset['recall'], subset['precision'], 
                  s=100, alpha=0.7, label=f'ngram={ngram}')
    
    ax.set_xlabel('Recall (Spam Detection Rate)')
    ax.set_ylabel('Precision (Spam Prediction Accuracy)')
    ax.set_title('Count-Min: Precision-Recall Trade-off')
    ax.legend()
    ax.grid(True, alpha=0.3)
    ax.set_xlim([0.8, 1.0])
    ax.set_ylim([0.8, 1.0])
    
    plt.tight_layout()
    plt.savefig('precision_recall_tradeoff.png', dpi=300, bbox_inches='tight')
    print("Saved: precision_recall_tradeoff.png")
    plt.close()


def plot_computational_efficiency():
    """
    Compare computational efficiency across different configurations.
    Experiment 5: Show time complexity trade-offs.
    """
    print("Plotting computational efficiency...")
    
    try:
        df = pd.read_csv('timing_results.csv')
    except FileNotFoundError:
        print("Warning: timing_results.csv not found")
        return
    
    fig, axes = plt.subplots(1, 2, figsize=(14, 6))
    
    # Plot 1: Time per email vs configuration
    ax = axes[0]
    
    x_pos = np.arange(len(df))
    colors = ['blue' if c == 'FeatureHashing' else 'green' 
             for c in df['classifier']]
    
    ax.bar(x_pos, df['time_per_email_ms'], color=colors, alpha=0.7)
    
    # Create labels
    labels = []
    for _, row in df.iterrows():
        clf_short = 'FH' if row['classifier'] == 'FeatureHashing' else 'CM'
        if row['num_hashes'] == 0:
            labels.append(f"{clf_short}\nng={int(row['ngram'])}\nlb={int(row['log_buckets'])}")
        else:
            labels.append(f"{clf_short}\nng={int(row['ngram'])}\nnh={int(row['num_hashes'])}\nlb={int(row['log_buckets'])}")
    
    ax.set_xticks(x_pos)
    ax.set_xticklabels(labels, fontsize=8)
    ax.set_ylabel('Time per Email (ms)')
    ax.set_title('Computational Time by Configuration')
    ax.grid(True, alpha=0.3, axis='y')
    
    # Add legend
    from matplotlib.patches import Patch
    legend_elements = [Patch(facecolor='blue', alpha=0.7, label='Feature Hashing'),
                      Patch(facecolor='green', alpha=0.7, label='Count-Min')]
    ax.legend(handles=legend_elements)
    
    # Plot 2: Total processing time
    ax = axes[1]
    
    ax.bar(x_pos, df['total_time_s'], color=colors, alpha=0.7)
    ax.set_xticks(x_pos)
    ax.set_xticklabels(labels, fontsize=8)
    ax.set_ylabel('Total Processing Time (seconds)')
    ax.set_title('Total Processing Time by Configuration')
    ax.grid(True, alpha=0.3, axis='y')
    
    plt.tight_layout()
    plt.savefig('computational_efficiency.png', dpi=300, bbox_inches='tight')
    print("Saved: computational_efficiency.png")
    plt.close()


def plot_pr_curve():
    """
    Plot Precision-Recall curve for threshold selection.
    Experiment 3: Shows how different thresholds affect precision and recall.
    """
    print("Plotting PR curve...")
    
    try:
        df = pd.read_csv('pr_curve.csv')
    except FileNotFoundError:
        print("Warning: pr_curve.csv not found")
        return
    
    # Get unique configurations
    configs = df.groupby(['classifier', 'ngram', 'log_buckets', 'num_hashes']).size().reset_index()[
        ['classifier', 'ngram', 'log_buckets', 'num_hashes']]
    
    # Create a plot for each configuration
    for _, config in configs.iterrows():
        clf_name = config['classifier']
        ng = int(config['ngram'])
        lb = int(config['log_buckets'])
        nh = int(config['num_hashes'])
        
        # Filter data for this configuration
        mask = ((df['classifier'] == clf_name) & 
                (df['ngram'] == ng) & 
                (df['log_buckets'] == lb) & 
                (df['num_hashes'] == nh))
        df_config = df[mask].copy()
        
        if len(df_config) == 0:
            continue
        
        config_label = f"{clf_name[:2]}_ng{ng}_lb{lb}"
        if nh > 0:
            config_label += f"_nh{nh}"
        
        print(f"  Plotting {clf_name} (ngram={ng}, log_buckets={lb}" + 
              (f", num_hashes={nh}" if nh > 0 else "") + ")...")
        
        fig, axes = plt.subplots(2, 2, figsize=(14, 12))
        
        # Plot 1: Classic PR Curve
        ax = axes[0, 0]
        ax.plot(df_config['recall'], df_config['precision'], linewidth=2, marker='o', markersize=3)
        ax.set_xlabel('Recall (True Positive Rate)')
        ax.set_ylabel('Precision')
        ax.set_title(f'Precision-Recall Curve\n{clf_name} (n={ng}, log_b={lb}' + 
                    (f', h={nh})' if nh > 0 else ')'))
        ax.grid(True, alpha=0.3)
        ax.set_xlim([0, 1])
        ax.set_ylim([0, 1])
        
        # Add F1 iso-lines
        f_scores = np.linspace(0.2, 0.9, num=8)
        for f_score in f_scores:
            x = np.linspace(0.01, 1)
            y = f_score * x / (2 * x - f_score)
            mask_valid = y >= 0
            if np.any(mask_valid):
                ax.plot(x[mask_valid], y[mask_valid], color='gray', alpha=0.3, 
                       linestyle='--', linewidth=0.5)
                y_pos = f_score * 0.9 / (2 * 0.9 - f_score + 0.001)
                if 0 <= y_pos <= 1:
                    ax.annotate(f'F1={f_score:.1f}', xy=(0.9, y_pos), 
                               fontsize=7, color='gray')
        
        # Plot 2: Threshold vs F1 Score
        ax = axes[0, 1]
        ax.plot(df_config['threshold'], df_config['f1'], linewidth=2, 
               marker='o', markersize=3, color='green')
        
        # Mark optimal threshold
        best_f1_idx = df_config['f1'].idxmax()
        best_threshold = df_config.loc[best_f1_idx, 'threshold']
        best_f1 = df_config.loc[best_f1_idx, 'f1']
        ax.axvline(best_threshold, color='red', linestyle='--', alpha=0.7, 
                  label=f'Optimal: {best_threshold:.2f}')
        ax.scatter([best_threshold], [best_f1], color='red', s=100, zorder=5)
        
        ax.set_xlabel('Threshold')
        ax.set_ylabel('F1 Score')
        ax.set_title('F1 Score vs Threshold')
        ax.legend()
        ax.grid(True, alpha=0.3)
        
        # Plot 3: Threshold vs Precision and Recall
        ax = axes[1, 0]
        ax.plot(df_config['threshold'], df_config['precision'], linewidth=2, 
               label='Precision', marker='o', markersize=3)
        ax.plot(df_config['threshold'], df_config['recall'], linewidth=2, 
               label='Recall', marker='o', markersize=3)
        ax.plot(df_config['threshold'], df_config['f1'], linewidth=2, 
               label='F1', marker='o', markersize=3)
        ax.axvline(best_threshold, color='red', linestyle='--', alpha=0.7, 
                  label=f'Optimal: {best_threshold:.2f}')
        ax.set_xlabel('Threshold')
        ax.set_ylabel('Score')
        ax.set_title('Metrics vs Threshold')
        ax.legend()
        ax.grid(True, alpha=0.3)
        
        # Plot 4: Confusion Matrix Components vs Threshold
        ax = axes[1, 1]
        total = df_config['tp'] + df_config['fp'] + df_config['tn'] + df_config['fn']
        ax.plot(df_config['threshold'], df_config['tp'] / total, linewidth=2, 
               label='True Positives', marker='o', markersize=2)
        ax.plot(df_config['threshold'], df_config['fp'] / total, linewidth=2, 
               label='False Positives', marker='o', markersize=2)
        ax.plot(df_config['threshold'], df_config['tn'] / total, linewidth=2, 
               label='True Negatives', marker='o', markersize=2)
        ax.plot(df_config['threshold'], df_config['fn'] / total, linewidth=2, 
               label='False Negatives', marker='o', markersize=2)
        ax.axvline(best_threshold, color='red', linestyle='--', alpha=0.7)
        ax.set_xlabel('Threshold')
        ax.set_ylabel('Proportion')
        ax.set_title('Confusion Matrix Components vs Threshold')
        ax.legend()
        ax.grid(True, alpha=0.3)
        
        plt.tight_layout()
        filename = f'pr_curve_{config_label}.png'
        plt.savefig(filename, dpi=300, bbox_inches='tight')
        print(f"  Saved: {filename}")
        print(f"  Optimal threshold for F1: {best_threshold:.3f} (F1={best_f1:.4f})")
        plt.close()
    
    # Create comparison plot if we have multiple configurations
    if len(configs) > 1:
        print("Creating comparison plot...")
        fig, axes = plt.subplots(1, 2, figsize=(14, 6))
        
        # Plot 1: PR Curve Comparison
        ax = axes[0]
        for _, config in configs.iterrows():
            clf_name = config['classifier']
            ng = int(config['ngram'])
            lb = int(config['log_buckets'])
            nh = int(config['num_hashes'])
            
            mask = ((df['classifier'] == clf_name) & 
                    (df['ngram'] == ng) & 
                    (df['log_buckets'] == lb) & 
                    (df['num_hashes'] == nh))
            df_config = df[mask].copy()
            
            label = f"{clf_name[:2]} (n={ng}, lb={lb}"
            if nh > 0:
                label += f", h={nh}"
            label += ")"
            
            ax.plot(df_config['recall'], df_config['precision'], 
                   linewidth=2, marker='o', markersize=2, label=label, alpha=0.8)
        
        ax.set_xlabel('Recall (True Positive Rate)')
        ax.set_ylabel('Precision')
        ax.set_title('PR Curve Comparison')
        ax.legend()
        ax.grid(True, alpha=0.3)
        ax.set_xlim([0, 1])
        ax.set_ylim([0, 1])
        
        # Plot 2: F1 Score Comparison
        ax = axes[1]
        for _, config in configs.iterrows():
            clf_name = config['classifier']
            ng = int(config['ngram'])
            lb = int(config['log_buckets'])
            nh = int(config['num_hashes'])
            
            mask = ((df['classifier'] == clf_name) & 
                    (df['ngram'] == ng) & 
                    (df['log_buckets'] == lb) & 
                    (df['num_hashes'] == nh))
            df_config = df[mask].copy()
            
            label = f"{clf_name[:2]} (n={ng}, lb={lb}"
            if nh > 0:
                label += f", h={nh}"
            label += ")"
            
            ax.plot(df_config['threshold'], df_config['f1'], 
                   linewidth=2, marker='o', markersize=2, label=label, alpha=0.8)
            
            # Mark optimal threshold
            best_f1_idx = df_config['f1'].idxmax()
            best_threshold = df_config.loc[best_f1_idx, 'threshold']
            best_f1 = df_config.loc[best_f1_idx, 'f1']
            ax.scatter([best_threshold], [best_f1], s=100, zorder=5)
        
        ax.set_xlabel('Threshold')
        ax.set_ylabel('F1 Score')
        ax.set_title('F1 Score vs Threshold Comparison')
        ax.legend()
        ax.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig('pr_curve_comparison.png', dpi=300, bbox_inches='tight')
        print("Saved: pr_curve_comparison.png")
        plt.close()


def generate_summary_table():
    """
    Generate a summary table of best configurations for the report.
    """
    print("Generating summary table...")
    
    try:
        df = pd.read_csv('hyperparameter_results.csv')
    except FileNotFoundError:
        print("Warning: hyperparameter_results.csv not found")
        return
    
    # Filter to keep only final step for each configuration
    df = df.groupby(['classifier', 'ngram', 'log_buckets', 'num_hashes']).apply(
        lambda x: x.loc[x['num_examples'].idxmax()]
    ).reset_index(drop=True)
    
    # Find best configuration for each metric and classifier
    summary = []
    
    for classifier in df['classifier'].unique():
        df_clf = df[df['classifier'] == classifier]
        
        for metric in ['precision', 'f1', 'accuracy']:
            best_idx = df_clf[metric].idxmax()
            best_row = df_clf.loc[best_idx]
            
            summary.append({
                'Classifier': classifier,
                'Optimized For': metric.upper(),
                'N-gram': int(best_row['ngram']),
                'Log Buckets': int(best_row['log_buckets']),
                'Num Hashes': int(best_row['num_hashes']) if best_row['num_hashes'] > 0 else 'N/A',
                'Precision': f"{best_row['precision']:.4f}",
                'Recall': f"{best_row['recall']:.4f}",
                'F1': f"{best_row['f1']:.4f}",
                'Accuracy': f"{best_row['accuracy']:.4f}"
            })
    
    summary_df = pd.DataFrame(summary)
    summary_df.to_csv('best_configurations.csv', index=False)
    print("Saved: best_configurations.csv")
    print("\nBest Configurations:")
    print(summary_df.to_string(index=False))


def plot_hyperparameter_learning_curves():
    """
    Plot learning curves showing impact of each hyperparameter.
    Shows how varying each parameter affects learning over time.
    """
    print("Plotting hyperparameter learning curves...")
    
    try:
        df = pd.read_csv('hyperparameter_results.csv')
    except FileNotFoundError:
        print("Warning: hyperparameter_results.csv not found")
        return
    
    metrics = ['accuracy', 'precision', 'recall', 'f1']
    
    # Feature Hashing - Vary ngram (log_buckets=14 constant)
    print("  Feature Hashing: varying ngram...")
    df_fh_ngram = df[(df['classifier'] == 'FeatureHashing') & (df['log_buckets'] == 14)]
    
    if len(df_fh_ngram) > 0:
        fig, axes = plt.subplots(2, 2, figsize=(14, 10))
        axes = axes.flatten()
        
        for idx, metric in enumerate(metrics):
            ax = axes[idx]
            for ngram in sorted(df_fh_ngram['ngram'].unique()):
                subset = df_fh_ngram[df_fh_ngram['ngram'] == ngram].sort_values('num_examples')
                ax.plot(subset['num_examples'], subset[metric], 
                       marker='o', markersize=3, label=f'ngram={int(ngram)}', linewidth=2)
            
            ax.set_xlabel('Number of Training Examples')
            ax.set_ylabel(metric.upper())
            ax.set_title(f'FH {metric.upper()} vs ngram (log_buckets=14)')
            ax.legend()
            ax.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig('hyperparam_learning_fh_ngram.png', dpi=300, bbox_inches='tight')
        print("  Saved: hyperparam_learning_fh_ngram.png")
        plt.close()
    
    # Feature Hashing - Vary log_buckets (ngram=2 constant)
    print("  Feature Hashing: varying log_buckets...")
    df_fh_buckets = df[(df['classifier'] == 'FeatureHashing') & (df['ngram'] == 2)]
    
    if len(df_fh_buckets) > 0:
        fig, axes = plt.subplots(2, 2, figsize=(14, 10))
        axes = axes.flatten()
        
        for idx, metric in enumerate(metrics):
            ax = axes[idx]
            for lb in sorted(df_fh_buckets['log_buckets'].unique()):
                subset = df_fh_buckets[df_fh_buckets['log_buckets'] == lb].sort_values('num_examples')
                ax.plot(subset['num_examples'], subset[metric], 
                       marker='o', markersize=3, label=f'log_buckets={int(lb)}', linewidth=2)
            
            ax.set_xlabel('Number of Training Examples')
            ax.set_ylabel(metric.upper())
            ax.set_title(f'FH {metric.upper()} vs log_buckets (ngram=2)')
            ax.legend()
            ax.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig('hyperparam_learning_fh_buckets.png', dpi=300, bbox_inches='tight')
        print("  Saved: hyperparam_learning_fh_buckets.png")
        plt.close()
    
    # Count-Min - Vary ngram (log_buckets=14, num_hashes=3 constant)
    print("  Count-Min: varying ngram...")
    df_cm_ngram = df[(df['classifier'] == 'CountMin') & 
                      (df['log_buckets'] == 14) & 
                      (df['num_hashes'] == 3)]
    
    if len(df_cm_ngram) > 0:
        fig, axes = plt.subplots(2, 2, figsize=(14, 10))
        axes = axes.flatten()
        
        for idx, metric in enumerate(metrics):
            ax = axes[idx]
            for ngram in sorted(df_cm_ngram['ngram'].unique()):
                subset = df_cm_ngram[df_cm_ngram['ngram'] == ngram].sort_values('num_examples')
                ax.plot(subset['num_examples'], subset[metric], 
                       marker='o', markersize=3, label=f'ngram={int(ngram)}', linewidth=2)
            
            ax.set_xlabel('Number of Training Examples')
            ax.set_ylabel(metric.upper())
            ax.set_title(f'CM {metric.upper()} vs ngram (log_buckets=14, num_hashes=3)')
            ax.legend()
            ax.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig('hyperparam_learning_cm_ngram.png', dpi=300, bbox_inches='tight')
        print("  Saved: hyperparam_learning_cm_ngram.png")
        plt.close()
    
    # Count-Min - Vary log_buckets (ngram=2, num_hashes=3 constant)
    print("  Count-Min: varying log_buckets...")
    df_cm_buckets = df[(df['classifier'] == 'CountMin') & 
                        (df['ngram'] == 2) & 
                        (df['num_hashes'] == 3)]
    
    if len(df_cm_buckets) > 0:
        fig, axes = plt.subplots(2, 2, figsize=(14, 10))
        axes = axes.flatten()
        
        for idx, metric in enumerate(metrics):
            ax = axes[idx]
            for lb in sorted(df_cm_buckets['log_buckets'].unique()):
                subset = df_cm_buckets[df_cm_buckets['log_buckets'] == lb].sort_values('num_examples')
                ax.plot(subset['num_examples'], subset[metric], 
                       marker='o', markersize=3, label=f'log_buckets={int(lb)}', linewidth=2)
            
            ax.set_xlabel('Number of Training Examples')
            ax.set_ylabel(metric.upper())
            ax.set_title(f'CM {metric.upper()} vs log_buckets (ngram=2, num_hashes=3)')
            ax.legend()
            ax.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig('hyperparam_learning_cm_buckets.png', dpi=300, bbox_inches='tight')
        print("  Saved: hyperparam_learning_cm_buckets.png")
        plt.close()
    
    # Count-Min - Vary num_hashes (ngram=2, log_buckets=14 constant)
    print("  Count-Min: varying num_hashes...")
    df_cm_hashes = df[(df['classifier'] == 'CountMin') & 
                       (df['ngram'] == 2) & 
                       (df['log_buckets'] == 14)]
    
    if len(df_cm_hashes) > 0:
        fig, axes = plt.subplots(2, 2, figsize=(14, 10))
        axes = axes.flatten()
        
        for idx, metric in enumerate(metrics):
            ax = axes[idx]
            for nh in sorted(df_cm_hashes['num_hashes'].unique()):
                subset = df_cm_hashes[df_cm_hashes['num_hashes'] == nh].sort_values('num_examples')
                ax.plot(subset['num_examples'], subset[metric], 
                       marker='o', markersize=3, label=f'num_hashes={int(nh)}', linewidth=2)
            
            ax.set_xlabel('Number of Training Examples')
            ax.set_ylabel(metric.upper())
            ax.set_title(f'CM {metric.upper()} vs num_hashes (ngram=2, log_buckets=14)')
            ax.legend()
            ax.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig('hyperparam_learning_cm_hashes.png', dpi=300, bbox_inches='tight')
        print("  Saved: hyperparam_learning_cm_hashes.png")
        plt.close()
    
    print("Hyperparameter learning curves complete!")


def main():
    """
    Run all visualization functions.
    """
    print("=" * 60)
    print("Spam Filter Experiment Visualization")
    print("=" * 60)
    
    # Create output directory for plots if needed
    Path('.').mkdir(exist_ok=True)
    
    # Generate all plots
    plot_learning_curves()
    plot_hyperparameter_heatmaps()
    plot_hyperparameter_learning_curves()
    plot_pr_curve()
    plot_computational_efficiency()
    generate_summary_table()
    
    print("\n" + "=" * 60)
    print("All visualizations complete!")
    print("=" * 60)


if __name__ == '__main__':
    main()

